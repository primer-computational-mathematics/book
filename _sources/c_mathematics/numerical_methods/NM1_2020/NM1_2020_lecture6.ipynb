{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods\n",
    "\n",
    "# Lecture 6: Numerical Linear Algebra II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "## I.   What are Matrices?\n",
    "## II.  Introduction to LU Decomposition\n",
    "## III. LU Decomposition Doolittle Algorithm\n",
    "## IV. LU Decomposition Implementation\n",
    "## V.  Partial Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives:\n",
    "\n",
    "* More on direct methods: LU decomposition\n",
    "* Doolittle's algorithm\n",
    "* Properties of lower-triangular matrices\n",
    "* Partial pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.   What are Matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have gained a bit of understanding about Matrix, it is important to think, for once, what exactly are matrices, determinantsm singularity and solvability of systems. And also, the various operations that Python has for Matrix. \n",
    "\n",
    "First things first, a bad thing with highschool mathematics is that it always makes matrices appear as something unrelated to the other fields of mathematics. But as you might have seen in the previous lectures, matrices can be used to solve systems of linear equations. Matrices have another bunch of <span style=\"badpun\">useful uses</span> in mathematics, as you might recall from Remote Sensing and GIS for image manipulations etc. You should therefore think of matrices as another tool we use in Mathematics, and a tool in mathematics that has been passed to the hands of other scientists who applied to all kinds of important scientific magic. \n",
    "\n",
    "Second, you should think of Matrices of something like a storage device, and stores values inside. Take our system of linear equations. Think about a poor chap, who before, the age of computers, had to solve everything by the power of hand and mind. Imagine given a system of linear equations for you to solve, and well, to solve this system of linear equations, the poor chap has to write $x,y,z$ again and again and again, just like all of you might have done in highschool. It's rather inconvenient, bothersome, wastes lots of paper and ink. What you need to solve any system of these linear equations, i.e. find the value of the unknowns, are the coefficients and the result of each system of linear equation on the RHS, and therefore, they thought, is there a way to write the coefficients, variables, and the results seprately, so that you don't have to repeatedly write stuff again and again? \n",
    "\n",
    "Well here comes matricesÔºÅMatrix $\\pmb{A}$ stores the coefficients, $\\pmb{x}$ stores the variables and $\\pmb{b}$ stores the results of the linear equations!\n",
    "\n",
    "$$\\pmb{Ax =b }$$\n",
    "\n",
    "You have already seent the capabilities of Gaussian elimination in solving equations. Now, when there are only 2 equations with 2 unknowns, Gaussian elimination is probably not the best method for solvng it. Rather, some of you should be able to solve these but inspection alone. But what happens when you have dozens of variables and dozens of unknowns, can you still solve it simply through inspection? Maybe some of you can, but I know that I cannot. The capabilities of matrices really shine when you have large complicated problems and it becomes difficult to keep track. \n",
    "\n",
    "Imagine a system with dozens of equation and dozens of unknown, and let's say that there exist a unique solution to our problem. Now, think about how times you would have to write $x,y,z$, if you well, did not have matrices, in order to solve the equation at hand. Basically, when solving these large equations, matrices make your life easier by decreasing the amount of things that you need to write. \n",
    "\n",
    "Moreover, think about your computer, which is neither omniscent and omnipotent, and sometimes confused at the simplest of things. While your Python today can easily deal with symbols through your Sympy libary, and can even solve system of linear equations quite easily, think about the first computers that were invented, with, well, very limited processing power and memory. For example, one of the earliest computers, ENIAC, whose first program was to study the feasibility of thermonuclear weapons. ENIAC had to do a lot of mathematical calculations, while having very limited memory to play with, thus requiring some way of organizing the information used for these mathematical calculations in such a way that it would take less memory. So, MATRIX. \n",
    "\n",
    "Even today, with Moore's Law having continued for so many years, we are still in a continous arms race between computational power and the problems we are seeking to solve. While the amount of memory you could equip a computer with has increased significantly since ENIAC, so has the power hungriness of the problems we seek to solve. Just look at the list of supercomputers in the world, would the government waste so much of the taxpayer's money on computational power they do not need? Thus, while we can increase memory, we should also try to make our methodology more memory efficient, so that they could solve really complex problem while armed with very limited memory. Making the problems we have into Matrices is a great way for saving lots of memory and computational power!\n",
    "\n",
    "Last lecture we introduced the calculation for determinants, but how do determinants determine if an equation is solvable or not? \n",
    "\n",
    "Let's start with the determinant's definition. From wikipedia, In linear algebra, the determinant is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the linear transformation described by the matrix. One of the most important properties that the determinant encodes is the solvability, meaning that finding the determinant can help us determine if the equaiton is solvable or not. \n",
    "\n",
    "Then, let's discuss solvability. When we try to solve an equation, we need information about the unknowns. If we are given enough information, the equation can be solved. If we are given too much information, or too little information, thus onverconstraining or underconstraining the problem, then the equation cannot be solved. You would encounter this especially when attempting to solve ODEs and PDEs in real world settings. Knowing how computationally expensive is to solve large scale problem, it is very important to know if the problem can be solved before beginning to solve the problem. Some of the most important topics in mathematics are about proving the solvability of certain forms or types of problems. \n",
    "\n",
    "Ok, so let's take a system of 2 linear equations, with 2 unknowns. If we make a graph, there could be 3 situations. The lines of the linear equation could intersect, the lines of the linear equations could not intersect,i.e. parallel or the two lines could simply be actually the same line in disguise, i.e. the line is still parallel, since a line is always parallel to itself, giving us infinitely many solutions. What we seek is the situation where the two lines intersect, and gives us a point, i.e. a unique solution, or the lines are not parallel.\n",
    "\n",
    "We could write our equation in general form as\n",
    "\n",
    "$$\n",
    "ax + by = e \\\\\n",
    "cx + dy = f\n",
    "$$\n",
    "\n",
    "If we wrote our equation in matrix form it would be \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a & b  \\\\\n",
    "c & d \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x  \\\\\n",
    "y \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "e  \\\\\n",
    "f \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We know the determinant for the matrix form above is \n",
    "\n",
    "$$\n",
    "\\pmb{det} = ad - bc\n",
    "$$\n",
    "\n",
    "Let's write the equation instead in slope - intercept form\n",
    "\n",
    "$$\n",
    "y = - \\frac{a}{b} x + \\frac{e}{b} \\\\\n",
    "y = - \\frac{c}{d} x + \\frac{f}{d}\n",
    "$$\n",
    "\n",
    "We know that there is unique solution when the lines are not parallel, and there is not a unique solution if the lines are not parallel. A line is parallel if the slope are equal, and not parallel if the slopes are not equal. We find the slopes as \n",
    "\n",
    "$$\n",
    "m_1 = - \\frac{a}{b}\\\\\n",
    "m_2 = - \\frac{c}{d}\n",
    "$$\n",
    "\n",
    "Let's consider the case when the lines are parallel, i.e. slopes are equal and let's manipulate it a little\n",
    "\n",
    "$$ \n",
    "- \\frac{a}{b} = - \\frac{c}{d} \\\\\n",
    "ad - bc = 0\n",
    "$$\n",
    "\n",
    "Let's consider the case when the lines are not parallel, i.e. slopes are not equal and let's manipulate it a little\n",
    "\n",
    "$$ \n",
    "- \\frac{a}{b} \\neq - \\frac{c}{d} \\\\\n",
    "ad - bc \\neq 0\n",
    "$$\n",
    "\n",
    "Hmmm, the $ad - bc$ looks oddly familiar, what could it be? \n",
    "\n",
    "\n",
    "\n",
    "Speaking about methodology improvement, we should also try to improve the algorithms we used. For example, is there some better algorithm than Gaussian elimination for solving linear equations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LU Decomposition\n",
    "\n",
    "Last week we developed and implemented the Gaussian elimination method to solve the linear matrix system ($A\\pmb{x}=\\pmb{b}$). \n",
    "\n",
    "This week we will consider a closely related solution method: *LU decomposition* or *LU factorisation*.\n",
    "\n",
    "Both are examples of *direct* solution methods - in the next lecture we will consider the alternate approach to solve linear systems, namely *iterative* or *indirect* methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LU decomposition - motivation\n",
    "\n",
    "Last week we implemented Gaussian elimination to solve the matrix system with *one* RHS vector $\\pmb{b}$.  \n",
    "\n",
    "We often have to deal with problems where we have multiple RHS vectors, all with the same matrix $A$.\n",
    "\n",
    "We could call the same code (e.g. our 'upper_triangle' and 'back_substitution' codes from last lecture) multiple times to solve all of these corresponding linear systems, but note that as the elimination algorithm is actually performing operations based upon (the same) $A$ each time, we would actually be wasting time repeating exactly the same operations - this is therefore clearly not an efficient solution to this problem.\n",
    "\n",
    "We could easily generalise our Gaussian elimination/back substitution algorithms to include multiple RHS column vectors in the augmented system, perform the same sequence of row operations (but now only once) to transform the matrix to upper-triangular form, and then perform back substitution on each of the transformed RHS vectors from the augmented system - cf. the use of Gaussian elimination to compute the inverse to a matrix by placing the identity on the right of the augmented system.\n",
    "\n",
    "However, it is often the case that each RHS vector depends on the solutions to the matrix systems obtained from some or all of the earlier RHS vectors, and so this generalisation would not work in this case. For example, our discrete system could be of the form\n",
    "\n",
    "$$A\\pmb{x}^{n+1} = \\pmb{b}(\\pmb{x}^n, \\ldots)$$\n",
    "\n",
    "where $\\pmb{x}^{n+1}$ is the numerical solution of a (ordinary or partial) differential equation at time level $n+1$, the RHS is a function of the the solution at the previous time level $n$ (and possibly other things such as focing terms, represented by the $\\ldots$ above. $A$ is then a discretisation of the differential equation written in the form that it maps the old solution to the new one. Time stepping this problem invovles solving the same linear system multiple times with different RHSs. \n",
    "\n",
    "[NB. this is true if our model is *linear*, in the nonlinear case $A$ would be updated evey time step]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so Gaussian elimination is nice, but way too inefficient, and basically, maybe we could do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU decomposition - theory\n",
    "\n",
    "To deal with this situation efficiently we *decompose* or *factorise* the matrix $A$ in such a way that it is cheap to compute a new solution vector $\\pmb{x}$ for any given RHS vector $\\pmb{b}$.  This decomposition involves a lower- and an upper-triangular matrix, hence the name LU decomposition. These matrices essentially *encode* the steps conducted in Gaussian elimination, so we don't have to explicilty conduct all of the operations again and again.\n",
    "\n",
    "Basically, we assume (well, there is actual rigorous mathematical proof that this can almost always be done) that our matrix **A** can be expressed as the matrix multiplication result of 2 other matrices, one in the upper triangle form, represented by the letter **U** and the other one in the lower triangle form, represented by the letter **L**. Since **U** is a upper triangle matrix, all entries below the diagonal are $0$, and for **L**, which is a lower triangle matrix, all entries above the diagonal are $0$. \n",
    "\n",
    "Mathematically, let's assume that we have already found/constructed a lower-triangular matrix ($L$ - where all entries above the diagonal are zero) and an upper-triangular matrix ($U$ - where all entries below the diagonal are zero) such that we can write\n",
    "\n",
    "$$ A = LU $$\n",
    "\n",
    "To prove this, let's say that \n",
    "\n",
    "In this case the matrix system we need to solve for $\\pmb{x}$ becomes\n",
    "\n",
    "$$ A\\pmb{x} = \\pmb{b} \\iff (LU)\\pmb{x} = L(U\\pmb{x}) = \\pmb{b} $$\n",
    "\n",
    "Notice that the matrix-vector product $U\\pmb{x}$ is itself a vector, let's call it $\\pmb{c}$ for the time-being (i.e. \n",
    "$\\pmb{c}=U\\pmb{x}$). Basically $U\\pmb{x}$ is a column like matrix. To illustrate\n",
    "\n",
    "Let's take a 3 by 3 matrix for example for $U$ and $x$ will correspndingly store 3 variables. \n",
    "\n",
    "We can generalize a 3 by 3 $U$ as below, with $a,b,c,d,e,f$ being arbitrary constants, remebering that $U$ is a upper triangle matrix, so all values below the diagonal are zero \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "U = \n",
    "\\begin{pmatrix}\n",
    "a & b & c \\\\\n",
    "0 & d & e \\\\\n",
    "0 & 0 & f\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "We also have our matrix $x$ where we store the variable (Yeah, I know it's hard getting used to $x$ as a matrix to store variables. If you are annoyed by the confusion, you could always replace the name $x$ with the name \"Matrix for storing variables\")\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pmb{x}_{a.k.aThe Matrix for Storing the Variables} = \n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Then, if we do $Ux$, we get\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pmb{Ux} = \n",
    "\\begin{pmatrix}\n",
    "a & b & c \\\\\n",
    "0 & d & e \\\\\n",
    "0 & 0 & f\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y  \\\\\n",
    "z  \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "ax + by + cz \\\\\n",
    "0x + dy + ez  \\\\\n",
    "0z + 0y + fz  \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "We understand that $\\pmb{Ux}$ is a column matrix, a.k.a a vector matrix, a.k.a a vector, which has 3 rows and 1 column. \n",
    "\n",
    "The above system then reads \n",
    "\n",
    "$$ L\\pmb{c} = \\pmb{b} $$\n",
    "\n",
    "where $L$ is a matrix and $\\pmb{c}$ is an unknown.  \n",
    "\n",
    "<span style=\"color:red\">Note: The $L$ and $U$ here are not same as the one in the previous lecture in Gaussian elimination. </span>\n",
    "\n",
    "Previous Lecture\n",
    "$$\n",
    "\\pmb{Upper Triangle Form_{Gaussian} x = B} \n",
    "$$\n",
    "\n",
    "This Lecture\n",
    "$$\n",
    "\\pmb{LU x = B} \n",
    "$$\n",
    "\n",
    "<span style=\"color:red\">Note: The $L$ and $U$ here are not same as the one in the previous lecture in Gaussian elimination. </span>\n",
    "\n",
    "As $L$ is in lower-triangular form we can use forward substitution (generalising the back subsitution algorithm/code we developed last week) to very easily find $\\pmb{c}$ in relatively few operations (we don't need to call the entire Gaussian elimination algorithm).\n",
    "\n",
    "Once we know $\\pmb{c}$ we then solve the second linear system \n",
    "\n",
    "$$ U\\pmb{x} = \\pmb{c} $$\n",
    "\n",
    "where now we can use the fact that $U$ is upper-triangular to use our back substitution algorithm again very efficiently to give the solution $\\pmb{x}$ we require.\n",
    "\n",
    "So for a given $\\pmb{b}$ we can find the corresponding $\\pmb{x}$ very efficiently, we can therefore do this repeatedly as each new $\\pmb{b}$ is given to us.\n",
    "\n",
    "Our challenge is therefore to find the matrices $L$ and $U$ allowing us to perform the decomposition $A=LU$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We assume that we have already found $\\pmb{L}$ and $\\pmb{U}$. Our objective is to find $\\pmb{x}$, i.e. we aim to find the unknowns stored in $\\pmb{x}$ which we will find by finding $\\pmb{x}$ We begin with the matrix form\n",
    "\n",
    "$$\n",
    "\\pmb{Ax =b}\n",
    "$$\n",
    "\n",
    "2. Since we know that $\\pmb{A = LU}$, we can write\n",
    "\n",
    "$$\n",
    "\\pmb{LUx = b}\n",
    "$$\n",
    "\n",
    "3. We know that $\\pmb{Ux}$ is a column vector, and we can set a column vector $\\pmb{c}$ and make it $\\pmb{c =Ux}$, thus we write\n",
    "\n",
    "$$\n",
    "\\pmb{Lc = b}\n",
    "$$\n",
    "\n",
    "4. Since we know $\\pmb{L}$, we will see how we find $\\pmb{L}$ and $\\pmb{U}$ later, we know that $\\pmb{L}$ is applied to column matrix and results in another column matrix, thus we can use back substitution to very easily find $\\pmb{c}$\n",
    "\n",
    "5. Once we have found $\\pmb{c}$, since we have $\\pmb{U}$, and we know that $\\pmb{x}$ is also a vector, then, using backward substitution again, we can easily find $\\pmb{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition Doolittle Algorithm\n",
    "\n",
    "Recall the comment above on the $L$ and $U$ matrices encoding the steps taken in Gaussian elimination.  Let's see how this works through the development of the so-called *Doolittle algorithm*.\n",
    "\n",
    "Let's consider an example matrix:\n",
    "\n",
    "$$\n",
    "  A=\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "{\\color{black}5} & {\\color{black}14} & {\\color{black}7} & {\\color{black}10}\\\\\n",
    "{\\color{black}20} & {\\color{black}77} & {\\color{black}41} & {\\color{black}48}\\\\\n",
    "{\\color{black}25} & {\\color{black}91} & {\\color{black}55} & {\\color{black}67}\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the first step of Gaussian elimination is to set the\n",
    "sub-diagonal elements in the first column to zero by subtracting multiples of\n",
    "the first row from each of the subsequent rows. \n",
    "\n",
    "For this example, using the symbolic notiation from last week\n",
    "this requires the row operations\n",
    "\n",
    "\\begin{align}\n",
    "Eq. (2) &\\leftarrow Eq. (2) - 1\\times Eq. (1)\\\\\n",
    "Eq. (3) &\\leftarrow Eq. (3) - 4\\times Eq. (1)\\\\\n",
    "Eq. (4) &\\leftarrow Eq. (4) - 5\\times Eq. (1)\\\\\n",
    "\\end{align}\n",
    "\n",
    "or mathematically, and for each element of the matrix (remembering that we are adding rows together - while one of\n",
    "the entries of a row will end up being zero, this also has the consequence of updating the rest of the values in that row, hence the iteration over $j$ below):\n",
    "\n",
    "\\begin{align}\n",
    "A_{2j} &\\leftarrow A_{2j} - \\frac{A_{21}}{A_{11}} A_{1j} = A_{2j} - \\frac{5}{5} \\times A_{1j}, \\quad j=1,2,3,4\\\\\n",
    "A_{3j} &\\leftarrow A_{3j} - \\frac{A_{31}}{A_{11}} A_{1j} = A_{3j} - \\frac{20}{5} \\times A_{1j}, \\quad j=1,2,3,4\\\\\n",
    "A_{4j} &\\leftarrow A_{4j} - \\frac{A_{41}}{A_{11}} A_{1j} = A_{4j} - \\frac{25}{5} \\times A_{1j}, \\quad j=1,2,3,4\\\\\n",
    "\\end{align}\n",
    "\n",
    "Notice that we can also write these exact operations on elements in terms of multiplication by a carefully chosen lower-triangular matrix where the non-zero's below the diagonal are restricted to a single column, e.g. for the example above\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{Orange}{-1}} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{Orange}{-4}} & {\\color{black}0} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{Orange}{-5}} & {\\color{black}0} & {\\color{black}0} & {\\color{black}1}\\\\   \n",
    "  \\end{bmatrix}\\qquad\\times\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}5} & {\\color{black}14} & {\\color{black}7} & {\\color{black}10}\\\\\n",
    "    {\\color{black}20} & {\\color{black}77} & {\\color{black}41} & {\\color{black}48}\\\\\n",
    "    {\\color{black}25} & {\\color{black}91} & {\\color{black}55} & {\\color{black}67}\\\\   \n",
    "  \\end{bmatrix}\\qquad=\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}{0}} & {\\color{blue}{7}} & {\\color{blue}{2}} & {\\color{blue}{1}}\\\\\n",
    "    {\\color{blue}{0}} & {\\color{blue}{49}} & {\\color{blue}{21}} & {\\color{blue}{12}}\\\\\n",
    "    {\\color{blue}{0}} & {\\color{blue}{56}} & {\\color{blue}{30}} & {\\color{blue}{22}}\\\\    \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The lower-triangular matrix (let's call this one $L_0$) is thus encoding the first step in Gaussian elimination.\n",
    "\n",
    "The next step involves taking the second row of the updated matrix as the new pivot (we will ignore partial pivoting for simplicity), and subtracting multiples of this row from those below in order to set the zeros below the diagonal in the second column to zero. \n",
    "\n",
    "Note that thios step can be achieved here with the multiplication by the following lower-triangular matrix (call this one $L_1$)\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{Orange}{-7}} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{Orange}{-8}} & {\\color{black}0} & {\\color{black}1}\\\\\n",
    "  \\end{bmatrix}\\qquad\\times\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}49} & {\\color{black}21} & {\\color{black}12}\\\\\n",
    "    {\\color{black}0} & {\\color{black}56} & {\\color{black}30} & {\\color{black}22}\\\\\n",
    "  \\end{bmatrix}\\qquad=\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{blue}{0}} & {\\color{blue}{7}} & {\\color{blue}{5}}\\\\\n",
    "    {\\color{black}0} & {\\color{blue}{0}} & {\\color{blue}{14}} & {\\color{blue}{14}}\\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "and finally for this example to get rid the of the leading 14 on the final row:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{Orange}{-2}} & {\\color{black}{1}}\\\\\n",
    "  \\end{bmatrix}\\qquad\\times\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}14} & {\\color{black}14}\\\\\n",
    "  \\end{bmatrix}\\qquad=\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{blue}{0}} & {\\color{blue}{4}}\\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "where this lower triangular matrix we will call $L_2$, and the RHS matrix is now in upper-triangular form as we expect from Gaussian elimination (call this $U$).\n",
    "\n",
    "In summary, the above operations (starting from $A$, first multipling pn the left by $L_0$, the multiplying the result on the left by $L_1$ and so on to arrive at an upper trianglualr matrix $U$) can be written as \n",
    "\n",
    "$$ L_2(L_1(L_0A)) = U $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Note that these special lower-triangular matrices $L_0, \\ldots$ are all examples of what is known as *atomic* lower-triangular matrices: a special form of unitriangular matrix - the diagonals are unity, where the off-diagonal entries are all zero apart from in a single column. </span>\n",
    "\n",
    "<span style=\"color:red\">Notice that for these special, simple matrices, their inverse is simply the original with the sign of those off-diagnonals changed:</span>\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrrrr}\n",
    "    1      & 0      & \\cdots      &        &              &        &         &   & 0 \\\\\n",
    "    0      & 1      & 0           & \\cdots &              &        &         &   & 0 \\\\\n",
    "    0      & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\\\\n",
    "           &        &             &   0    &   1          &  0     &         & &  &  \\\\           \n",
    "           &        &             &   0    &   l_{i+1,i}  &  1     &  \\ddots &   &  &  \\\\  \n",
    "           &        &             &   0    &   l_{i+2,i}  &  0     &  \\ddots &   & &  \\\\  \n",
    "           &        &             & \\vdots &   \\vdots     & \\vdots &  \\ddots &   & 0 &  \\\\               \n",
    "    0      & \\cdots &             & 0      &   l_{n,i}    & 0      &  \\cdots & 0 & 1 &  \\\\    \n",
    "\\end{array}\n",
    "\\right]^{-1}\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrrrr}\n",
    "    1      & 0      & \\cdots      &        &              &        &         &   & 0 \\\\\n",
    "    0      & 1      & 0           & \\cdots &              &        &         &   & 0 \\\\\n",
    "    0      & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\\\\n",
    "           &        &             &   0    &   1          &  0     &         & &  &  \\\\           \n",
    "           &        &             &   0    &   -l_{i+1,i}  &  1     &  \\ddots &   &  &  \\\\  \n",
    "           &        &             &   0    &   -l_{i+2,i}  &  0     &  \\ddots &   & &  \\\\  \n",
    "           &        &             & \\vdots &   \\vdots     & \\vdots &  \\ddots &   & 0 &  \\\\               \n",
    "    0      & \\cdots &             & 0      &   -l_{n,i}    & 0      &  \\cdots & 0 & 1 &  \\\\    \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 6.1: lower-triangular matrices</span>\n",
    "\n",
    "Convince yourselves of the following facts:\n",
    "\n",
    "* The above statement on what the inverse of the special $L$ matrices is - check on a small example.\n",
    "\n",
    "\n",
    "* The multiplication of arbitrary lower-triangular square matrices is also lower-triangular.\n",
    "\n",
    "\n",
    "* $L_2(L_1(L_0A)) = U \\implies A = L_0^{-1}(L_1^{-1}(L_2^{-1}U))$\n",
    "\n",
    "\n",
    "* and hence that $A=LU$ where $U$ is the upper-triangular matrix found at the end of Guassian elimination, and where $L$ is the \n",
    "following  matrix\n",
    "$$ L = L_0^{-1}L_1^{-1}L_2^{-1} $$\n",
    "\n",
    "\n",
    "* Finally, compute this product of these lower-triangular matrices to show that \n",
    "$$L = \n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}{1}} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}{4}} & {\\color{black}7} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{black}{5}} & {\\color{black}8} & {\\color{black}2} & {\\color{black}1}\\\\   \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "i.e. that the multiplication of these individual atomic matrices (importantly in this order) simply merges the entries from the non-zero columns of each atomic matrix, and hence is both lower-triangular, as well as trivial to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> So now, by this slide, we have found $\\pmb{L}$, Hooray!  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition Implementation\n",
    "\n",
    "So we can build an LU code easily from our Gaussian elimination code from last lecture which already works out and performs these tasks.  \n",
    "\n",
    "The final $U$ matrix we need here is as was already constructed through Gaussian elimination, the entries of $L$ we need are simply the ${A_{ik}}/{A_{kk}}$ multipliers we computed as part of the elimination, but threw away previously.\n",
    "\n",
    "For a given pivot row $k$, for each of these multipliers (for every row below the pivot), as we compute them we know that we are going to transform the augmented matrix in order to achieve a new zero below the diagonal - we can store each multiplier in this position before moving on to the following row, we implicitly know that the diagonals of $L$ will be unity and so don't need to store these (and noting that we don't actually have a space for them anyway!). We then move on to the following pivot row, replacing the zeros in the corresponding column we are zero'ing, but again using the now spare space to store the multipliers.\n",
    "\n",
    "For example, for the case above \n",
    "\n",
    "$$ A = \n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}5} & {\\color{black}14} & {\\color{black}7} & {\\color{black}10}\\\\\n",
    "    {\\color{black}20} & {\\color{black}77} & {\\color{black}41} & {\\color{black}48}\\\\\n",
    "    {\\color{black}25} & {\\color{black}91} & {\\color{black}55} & {\\color{black}67}\\\\   \n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}{1}} & {\\color{black}{14 - 1\\times7}} & {\\color{black}{7-1\\times5}} & {\\color{black}{10-1\\times9}}\\\\\n",
    "    {\\color{blue}{4}} & {\\color{black}{77 - 4\\times7}} & {\\color{black}{41-4\\times5}} & {\\color{black}{48-4\\times9}}\\\\\n",
    "    {\\color{blue}{5}} & {\\color{black}{91 - 5\\times7}} & {\\color{black}{55-5\\times5}} & {\\color{black}{67-5\\times9}}\\\\ \n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}{1}} & {\\color{black}{7}} & {\\color{black}{2}} & {\\color{black}{1}}\\\\\n",
    "    {\\color{blue}{4}} & {\\color{black}{49}} & {\\color{black}{21}} & {\\color{black}{12}}\\\\\n",
    "    {\\color{blue}{5}} & {\\color{black}{56}} & {\\color{black}{30}} & {\\color{black}{22}}\\\\    \n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}{1}} & {\\color{black}{7}} & {\\color{black}{2}} & {\\color{black}{1}}\\\\\n",
    "    {\\color{blue}{4}} & {\\color{blue}{7}} & {\\color{black}{21-2\\times7}} & {\\color{black}{12-1\\times7}}\\\\\n",
    "    {\\color{blue}{5}} & {\\color{blue}{8}} & {\\color{black}{30-2\\times8}} & {\\color{black}{22-1\\times8}}\\\\    \n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}1} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{blue}4} & {\\color{blue}{7}} & {\\color{black}{7}} & {\\color{black}{5}}\\\\\n",
    "    {\\color{blue}5} & {\\color{blue}{8}} & {\\color{black}{14}} & {\\color{black}{14}}\\\\\n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}1} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{blue}4} & {\\color{blue}{7}} & {\\color{black}{7}} & {\\color{black}{5}}\\\\\n",
    "    {\\color{blue}5} & {\\color{blue}{8}} & {\\color{blue}{2}} & {\\color{black}{14-2\\times5}}\\\\\n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}1} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{blue}4} & {\\color{blue}7} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{blue}5} & {\\color{blue}8} & {\\color{blue}{2}} & {\\color{black}{4}}\\\\\n",
    "  \\end{bmatrix}\n",
    "  = [\\color{blue}L\\backslash U]\n",
    "$$\n",
    "\n",
    "We find that \n",
    "\n",
    "$$\n",
    "U =\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}0} & {\\color{black}{4}}\\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 6.2: LU decomposition</span>\n",
    "\n",
    "Starting from your Gaussian elimination code produce a new code to compute the LU decomposition of a matrix. First, store $L$ and $U$ in two different matrices; you could then try a version where you store the entries of both $L$ and $U$ in $A$ as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> So now, by this point, we have found $\\pmb{U}$, Hooray!  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> After finding both $\\pmb{L}$ and $\\pmb{U}$, we could use our aforementioned method to solve equations!   </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  V.  Partial Pivoting\n",
    "\n",
    "At the end of last week we commented that a problem could occur in a numeical implementation of our algorithm if we had a situation where the $A_{kk}$ we divide through by in the Gaussian elimination and/or back substitution algorithms might be zero (or close to zero).\n",
    "\n",
    "Using Gaussian elimination as an example, let's again consider the algorithm mid-way working on an arbitrary matrix system, i.e. assume that the first $k$ rows have already been transformed into upper-triangular form, while the equations/rows below are not yet in this form:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrr|r}\n",
    "    A_{11} & A_{12} & A_{13} & \\cdots & A_{1k}  & \\cdots & A_{1n} & b_1 \\\\\n",
    "    0      & A_{22} & A_{23} & \\cdots & A_{2k} & \\cdots & A_{2n} & b_2 \\\\\n",
    "    0      & 0      & A_{33} & \\cdots & A_{3k}  & \\cdots & A_{3n} & b_3 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\hdashline    \n",
    "    0      & 0      & 0      & \\cdots & A_{kk}  & \\cdots & A_{kn} & b_k \\\\    \n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "    0      & 0      & 0      & \\cdots & A_{nk}  & \\cdots & A_{nn} & b_n \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Note we have drawn the horizontal dashed line one row higher, as we are not going to blindly asssume that it is wise to take the current row $k$ as the pivot row, and $A_{kk}$ as the so-called pivot element.\n",
    "\n",
    "*Partial pivoting* selects the best pivot (row or element) as the one where the $A_{ik}$ (for $i\\ge k$) value is largest (relative to the other values of components in its own row $i$), and then swaps this row with the current $k$ row.\n",
    "\n",
    "To generalise our codes above we would simply need to search for this row, and perform the row swap operation.\n",
    "\n",
    "Python's `scipy.linalg` library has its own implementation of the LU decomposition, that uses partial pivoting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's illustrate partial pivoting with an example\n",
    "\n",
    "Suppose that we have the matrix $\\pmb{A}$ from before\n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    5 & 14 & 7 & 10 \\\\\n",
    "    20 & 77 &41 & 48 \\\\\n",
    "    25 & 91 & 55 & 67 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And then we do some timeskipping and skip to partway through\n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    0 & 7 & 2 & 1 \\\\\n",
    "    0 & 49 &21 & 12 \\\\\n",
    "    0 & 56 & 30 & 22 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, magic happens for the sake of illustrating the Partial Pivoting\n",
    "Magic magic poof poof and magically $7$ becomes $0$\n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    0 & 0 & 2 & 1 \\\\\n",
    "    0 & 49 &21 & 12 \\\\\n",
    "    0 & 56 & 30 & 22 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose that the matrix storing our unknowns is \n",
    "\n",
    "$$\n",
    "\\pmb{x} = \n",
    "\\begin{bmatrix}\n",
    "  x \\\\\n",
    "  y \\\\\n",
    "  z \\\\\n",
    "  letter\\;\\;after\\;\\;z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the matrix storing the results of our equations, after all of the Gaussian elimination previously that have modified the matrix $\\pmb{b}$\n",
    "\n",
    "$$\n",
    "\\pmb{b} = \n",
    "\\begin{bmatrix}\n",
    "  b_1 \\\\\n",
    "  b_2 \\\\\n",
    "  b_3 \\\\\n",
    "  b_4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Writing this in equation this would be \n",
    "\n",
    "$$\n",
    "5x + 7y + 5z + 9(letter\\;\\;after\\;\\;z) = b_1 \\\\\n",
    "0x + 0y + 2z + 1(letter\\;\\;after\\;\\;z) = b_2 \\\\\n",
    "0x + 49y + 21z + 12(letter\\;\\;after\\;\\;z) = b_3  \\\\\n",
    "0x + 56y + 30z + 22(letter\\;\\;after\\;\\;z) = b_4\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we are going to use equation 2 and equation 3\n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    0 & 0 & 2 & 1 \\\\\n",
    "    0 & 49 &21 & 12 \\\\\n",
    "    0 & 56 & 30 & 22 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The coefficient of the second unknown in the second equation is 0. The coefficient of the second unknown in the third equation is 49. Thus, I will divide the coefficient of the 3rd equation with the coefficient of the 2nd equation, and then I would multiply the coefficient of the 3rd equation by the quotient. As such, I will divide $49/0$, and then, yup I will get an error since you cannot divide by $0$. It does not neccesarily have to be $0$ to give errors, even if it is very close to $0$, it will still give an error. \n",
    "Thus what can I do? \n",
    "\n",
    "Well, I need two equations. I cannot use the equation 2 because there is a $0$. Then, I will use equation 3 and equation 4. Therefore, let's swap the position of equation 4 and equation 2. \n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    0 & 56 & 30 & 22 \\\\\n",
    "    0 & 49 &21 & 12 \\\\\n",
    "    0 & 0 & 2 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<span style=\"color:red\">Remember: when you swap rows in matrix $\\pmb{A}$, you must also swap rows in the matrix $\\pmb{b}$. Matrix $\\pmb{x}$ does not have to swapped since the order of the variables has not changed. </span>\n",
    "\n",
    "The matrix storing our unknowns remains\n",
    "\n",
    "$$\n",
    "\\pmb{x} = \n",
    "\\begin{bmatrix}\n",
    "  x \\\\\n",
    "  y \\\\\n",
    "  z \\\\\n",
    "  letter\\;\\;after\\;\\;z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the matrix storing the results of our equations\n",
    "\n",
    "$$\n",
    "\\pmb{b} = \n",
    "\\begin{bmatrix}\n",
    "  b_1 \\\\\n",
    "  b_4 \\\\\n",
    "  b_3 \\\\\n",
    "  b_2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Writing this in equation this would be \n",
    "\n",
    "$$\n",
    "5x + 7y + 5z + 9(letter\\;\\;after\\;\\;z) = b_1 \\\\\n",
    "0x + 56y + 30z + 22(letter\\;\\;after\\;\\;z) = b_4 \\\\\n",
    "0x + 49y + 21z + 12(letter\\;\\;after\\;\\;z) = b_3  \\\\\n",
    "0x + 0y + 2z + 1(letter\\;\\;after\\;\\;z) = b_2 \n",
    "$$\n",
    "\n",
    "Inspeacing this equation, we see that the equation is essentially the same as it was before the swap, so we can say that our action to swap it is a valid action. I have rewritten the equation before the swap as reference\n",
    "\n",
    "$$\n",
    "5x + 7y + 5z + 9(letter\\;\\;after\\;\\;z) = b_1 \\\\\n",
    "0x + 0y + 2z + 1(letter\\;\\;after\\;\\;z) = b_2 \\\\\n",
    "0x + 49y + 21z + 12(letter\\;\\;after\\;\\;z) = b_3  \\\\\n",
    "0x + 56y + 30z + 22(letter\\;\\;after\\;\\;z) = b_4\n",
    "$$\n",
    "\n",
    "Now let's continue, after we finished swapping, and we will focus only on getting to the upper triangle form, and only on matrix $\\pmb{A}$. The matrix $\\pmb{b}$ will also change accordingly as detailed in the previous lecture, but let's focus on matrix $\\pmb{A}$\n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    0 & 56 & 30 & 22 \\\\\n",
    "    0 & 56 & 24 & 96/7 \\\\\n",
    "    0 & 0 & 2 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "A = \n",
    "  \\begin{bmatrix}\n",
    "    5 & 7 & 5 &  9 \\\\\n",
    "    0 & 56 & 30 & 22 \\\\\n",
    "    0 & 0 & -6 & 96/7 -22 \\\\\n",
    "    0 & 0 & 2 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We will not encounter the problem with dividing by zero anymore, at least for this matrix ! \n",
    "\n",
    "Note : this matrix was nice and only required one swap. But if your matrix has lots of $0$ or very small numbers, then maybe you might need more swaps. You might also want to nondimensionalize that matrix! That is, of course assuming that the equations contain enough information to actually allow you to solve the equation. You could use the determinant, or the Gauss-Jordan in the previous lecture to check if there is actually a solution. You should always check the solvability of an equation before solving it, since trying to solve an equation which cannot be solved is wasted effort. <span class=\"personal experience\"> You may spend days debugging your code and scratching your head as to what went wrong, when in fact, the equations you are solving may not even be able to give a solution, i.e. ill conditioned </span> \n",
    "\n",
    "Note: You could use conditionals to control the swapping or the skipping of rows. Although the example here used $0$, it could be a very small number too, and not neccesary $0$ that could cause the problem, so maybe it would be nice to compare the conditional to a certain threshold to decided whether to swap or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  7.  5.  9.]\n",
      " [ 5. 14.  7. 10.]\n",
      " [20. 77. 41. 48.]\n",
      " [25. 91. 55. 67.]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[ 5., 7.,   5.,  9.],\n",
    "               [ 5., 14.,  7., 10.],\n",
    "               [20., 77., 41., 48.],\n",
    "               [25., 91. ,55., 67.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "P,L,U=sl.lu(A)\n",
    "\n",
    "# P here is a 'permutation matrix' that performs swaps based upon partial pivoting\n",
    "print(P)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.2         1.          0.          0.        ]\n",
      " [ 0.8        -0.375       1.          0.        ]\n",
      " [ 0.2         0.375       0.33333333  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# the lower-triangular matrix\n",
    "print(L)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.          91.          55.          67.        ]\n",
      " [  0.         -11.2         -6.          -4.4       ]\n",
      " [  0.           0.          -5.25        -7.25      ]\n",
      " [  0.           0.           0.           0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# the upper-triangular matrix\n",
    "print(U)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  7.  5.  9.]\n",
      " [ 5. 14.  7. 10.]\n",
      " [20. 77. 41. 48.]\n",
      " [25. 91. 55. 67.]]\n"
     ]
    }
   ],
   "source": [
    "# double check that P*L*U does indeed equal A\n",
    "print(P @ L @ U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the form of $P$ above, we can re-order the rows in advance and consider the LU decomposition of the matrix where $P=I$, as below. As we haven't bothered implementing pivoting ourselves, check that your LU implementation recreates the $A$, $L$ and $U$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25. 91. 55. 67.]\n",
      " [ 5.  7.  5.  9.]\n",
      " [20. 77. 41. 48.]\n",
      " [ 5. 14.  7. 10.]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[25. ,91. ,55. ,67.],\n",
    "               [ 5.,  7.,  5.,  9.], \n",
    "               [20., 77., 41., 48.],\n",
    "               [ 5., 14., 7.,  10.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "P,L,U=sl.lu(A)\n",
    "# P now should be the identity as pivoting no longer actually actions any row swaps with this A\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.2         1.          0.          0.        ]\n",
      " [ 0.8        -0.375       1.          0.        ]\n",
      " [ 0.2         0.375       0.33333333  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.          91.          55.          67.        ]\n",
      " [  0.         -11.2         -6.          -4.4       ]\n",
      " [  0.           0.          -5.25        -7.25      ]\n",
      " [  0.           0.           0.           0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25. 91. 55. 67.]\n",
      " [ 5.  7.  5.  9.]\n",
      " [20. 77. 41. 48.]\n",
      " [ 5. 14.  7. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(P @ L @ U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 6.3: Partial pivoting</span>\n",
    "\n",
    "Implement partial pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "427.513px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
