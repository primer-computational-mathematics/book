{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(General_Back_Propagation)=\n",
    "# Chapter 10 -- General Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/singleNeuronStructure3.PNG\" width=\"800\">\n",
    "<center>Figure 1.14: The3blue1brown does a pretty good animation already:\n",
    "    https://www.youtube.com/watch?v=tIeHLnjs5U8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the general format, let's have even one more layer...four layers (figure 1.14). So we have one input layer, two hidden layers and one output layer. To simplify the problem, we have only one neuron in each layer (one weight per layer, e.g. $w_1$,$w_2$,...), with $b=0$. And $C$ is some cost function. The output in the output layer,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "a^{(N)} = \\boldsymbol{y} = \\sigma(w^{(N)}*a^{(N-1)})\n",
    "\\end{equation}\n",
    "where $N=3$ in this example of figure 1.14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "a^{(3)} = \\boldsymbol{y} = \\sigma(w^{(3)}*a^{(2)})\n",
    "\\end{equation}\n",
    "where $a^{(2)} = \\sigma(h_2)$, and it determines the cost function (simplified to $C=(\\boldsymbol{y}-y)^2$). If the actual output $\\boldsymbol{y}$ from the network is close to the desired output $y$, then the cost will be low, while if it's far away, the cost will be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's remember that we have used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^{(N)} = w^{(N)}*a^{(N-1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make things simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I use $w_n$ and $w^{(N)}$ interchangeably, but they are the same in the single neuron net example. The notation of $w^{(N)}$ is for a more complicated network we cover later. Anyway, let's study the gradient $\\frac{\\delta C}{\\delta w_1}$ associated to the first hidden neuron. If we change the weight $w_1$ by a tiny nudge $\\Delta w_1$. That will set off a cascading series of changes in the rest of the network. First, it causes a change $\\Delta a_1$ ($a_1 = \\sigma(h_1)$) in the output from the first hidden neuron. That, in turn, will cause a change $\\Delta h_2$ in the weighted input to the second hidden neuron. Then a change $\\Delta a_2$ in the output from the second hidden neuron. And so on, all the way through to a change $\\Delta C$ in the cost at the output. We have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta w_1} \\approx \\frac{\\Delta C}{\\Delta w_1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that we can figure out an expression for the gradient $\\frac{\\delta C}{\\delta w_1}$ by carefully tracking the effect of each step in this cascade. To do this, let's think about how $\\Delta w_1$ causes the output a1 from the first hidden neuron to change. We have $a_1=\\sigma(h1)=\\sigma(w_1*x)$, so the change in $a_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta a_1 \\approx \\frac{\\delta \\sigma(h_1)}{\\delta w_1} * \\Delta w_1 =\\frac{\\delta \\sigma(h_1)}{\\delta h_1}*\\frac{\\delta h_1}{\\delta w_1}*\\Delta w_1 =   \\sigma^{'}(h_1)*x*\\Delta w_1 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That $\\sigma^{'}(h_1)$ term converts a change $\\Delta w_1$ in the weight into a change $\\Delta a_1$ in the output activation. That change $\\Delta a_1$ in turn causes a change in the weighted input $h_2=w_2*a_1$ to the second hidden neuron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta h_2 \\approx \\frac{\\delta h_2}{\\delta a_1} * \\Delta a_1 = w_2*\\Delta a_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining our expressions for $\\Delta h_2$ and $\\Delta a_1$, we see how the change in the weight $w_1$ propagates along the network to affect $h_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta h_2 \\approx w_2* \\sigma^{'}(h_1)*x*\\Delta w_1 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change in $h_2$ can result in a change in $a_2 = \\sigma(h2)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta a_2 = \\Delta \\sigma(h_2) \\approx \\frac{\\delta a_2}{h_2} * \\Delta h_2 =  \\sigma^{'}(h_2) * \\Delta h_2 =\\sigma^{'}(h_2) * w_2* \\sigma^{'}(h_1)*\\Delta w_1 \n",
    "\\end{equation}\n",
    "which will cause a change in $h_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta h_3 \\approx \\frac{\\delta h_3}{\\delta a_2} * \\Delta a_2 = w_3 * \\Delta a_2 = w_3 * \\sigma^{'}(h_2) * w_2* \\sigma^{'}(h_1)*x*\\Delta w_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change in $h_3$ will cause a change in $\\sigma(h_3)$, also known as $\\boldsymbol{y}$, the prediction result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta \\boldsymbol{y} = \\Delta \\sigma(h_3) \\approx \\frac{\\delta y}{h_3} * \\Delta h_3 =\\sigma^{'}(h_3) * \\Delta h_3 = \\sigma^{'}(h_3) * w_3 * \\sigma^{'}(h_2) * w_2* \\sigma^{'}(h_1)*x*\\Delta w_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change in the prediction value $\\boldsymbol{y}$ will cause a change in the cost function, $\\Delta C$, making it bigger or smaller (of course we want it to be as small as possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta C \\approx \\frac{\\delta C}{\\delta \\boldsymbol{y}} * \\Delta \\boldsymbol{y} = \\frac{\\delta C}{\\delta \\boldsymbol{y}}*\\sigma^{'}(h_3) * w_3 * \\sigma^{'}(h_2) * w_2* \\sigma^{'}(h_1)*x*\\Delta w_1\n",
    "\\label{deltaC}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep going in this fashion, tracking the way changes propagate through the rest of the network, in theory to the $n^{th}$ layer. After dividing $\\Delta w_1$ at the both sides in equation \\ref{deltaC}, the resulting gradient for the $w_1$ in this four layers network should be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta w_1} =  x*\\sigma^{'}(h_1)*w_2*\\sigma^{'}(h_2)*w_3*\\sigma^{'}(h_3)*\\frac{\\delta C}{\\delta \\boldsymbol{y}}\n",
    "\\label{dCdw1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta w_1} = \\frac{\\delta h_1}{\\delta w_1}*\\frac{\\delta a^{(1)}}{\\delta h_1}*\\frac{\\delta h_2}{\\delta a^{(1)}}*\\frac{\\delta a^{(2)}}{\\delta h_2}*\\frac{\\delta h_3}{\\delta a^{(2)}}*\\frac{\\delta a^{(3)}}{\\delta h_3}*\\frac{\\delta C}{\\delta a^{(3)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be reaffirming our idea of defining the `Error Term' (by figure 1.15) for the efficiency of our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/singleNeuronStructure4.PNG\" width=\"800\">\n",
    "<center> Figure 1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at just the last two neurons -- $a^{(N-1)}$ and $a^{(N)}$, then we have the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta h_3}{\\delta a^{(2)}}*\\frac{\\delta a^{(3)}}{\\delta h_3}*\\frac{\\delta C}{\\delta a^{(3)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is investigating the amount of change of $C$ due to a tiny nudge of $a^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is helpful because now we can just keep iterating this chain rule idea backwards to see how sensitive the cost to the previous weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a little bit more complicated when we have more than one neuron in each layer because we have more indices to keep track of. Let's take a look at the last two layers in a multiple neuron structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multipleNeuronStructure1.PNG\" width=\"800\">\n",
    "<center> Figure 1.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change of $h_1^{(3)}$ or $h_2^{(3)}$ in the output layer will change the cost $C$, and we define such gradient as $\\delta^{(N)}_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta h_1^{(3)}} = \\delta^{(3)}_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta h_2^{(3)}} = \\delta^{(3)}_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta h_j^{(N)}} = \\delta^{(N)}_j = \\frac{\\delta C}{\\delta a^{(N)}_j} \\frac{a^{(N)}_j}{\\delta h_j^{(N)}} = \\frac{\\delta C}{\\delta a^{(N)}_j} \\sigma^{'}(h_j^{(N)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to rewrite the equation in a matrix-based form, as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\delta^{(N)} = \\frac{\\delta C}{\\delta a^{(N)}}\\odot \\sigma^{'}(h^{(N)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the change in the $h_j^{(N-1)}$ in the $N-1$ layer will change $a^{(N-1)}_j$ and then results in a change in $a^{(N)}_j$, which will cause the change in the cost $C$. In specific (red neuron in figure 1.16), we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta h_j^{(N-1)}} = \\frac{\\delta C}{\\delta h_2^{(2)}} = \\frac{\\delta C}{\\delta a^{(3)}_1} * \\frac{\\delta a^{(3)}_1}{\\delta h_1^{(3)}} * \\frac{\\delta h_1^{(3)}}{\\delta a^{(2)}_2}*\\frac{\\delta a^{(2)}_2}{\\delta h_2^{(2)}} +  \\frac{\\delta C}{\\delta a^{(3)}_2} * \\frac{\\delta a^{(3)}_2}{\\delta h_2^{(3)}} * \\frac{\\delta h_2^{(3)}}{\\delta a^{(2)}_2}*\\frac{\\delta a^{(2)}_2}{\\delta h_2^{(2)}} \n",
    "\\label{dCdh22}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta h_1^{(2)}} = \\frac{\\delta C}{\\delta a^{(3)}_1} * \\frac{\\delta a^{(3)}_1}{\\delta h_1^{(3)}} * \\frac{\\delta h_1^{(3)}}{\\delta a^{(2)}_1}*\\frac{\\delta a^{(2)}_1}{\\delta h_1^{(2)}} +  \\frac{\\delta C}{\\delta a^{(3)}_2} * \\frac{\\delta a^{(3)}_2}{\\delta h_2^{(3)}} * \\frac{\\delta h_2^{(3)}}{\\delta a^{(2)}_1}*\\frac{\\delta a^{(2)}_1}{\\delta h_1^{(2)}} \n",
    "\\label{dCdh12}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta h_3^{(2)}} = \\frac{\\delta C}{\\delta a^{(3)}_1} * \\frac{\\delta a^{(3)}_1}{\\delta h_1^{(3)}} * \\frac{\\delta h_1^{(3)}}{\\delta a^{(2)}_3}*\\frac{\\delta a^{(2)}_3}{\\delta h_3^{(2)}} +  \\frac{\\delta C}{\\delta a^{(3)}_2} * \\frac{\\delta a^{(3)}_2}{\\delta h_2^{(3)}} * \\frac{\\delta h_2^{(3)}}{\\delta a^{(2)}_3}*\\frac{\\delta a^{(2)}_3}{\\delta h_3^{(2)}} \n",
    "\\label{dCdh32}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ErrorTermN_1.PNG\" width=\"800\">\n",
    "<center> Figure 1.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define such gradient in equations \\ref{dCdh22}, \\ref{dCdh12} and \\ref{dCdh32} by $\\delta^{(N-1)}$ in a vector form with $\\delta^{(N)}$ (shown in figure 1.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{bmatrix} \\delta_1^{(2)}\\\\\\delta_2^{(2)}\\\\\\delta_3^{(2)}\\end{bmatrix}\n",
    "= \\begin{bmatrix} w^{(3)}_{11} & w^{(3)}_{21} \\\\w^{(3)}_{12} & w^{(3)}_{23} \\\\w^{(3)}_{13} & w^{(3)}_{23} \\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix} \\delta_1^{(3)}\\\\\\delta_2^{(3)}\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix} \\sigma^{'}(h_1^{(2)})\\\\\\sigma^{'}(h_2^{(2)})\\\\\\sigma^{'}(h_3^{(2)})\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This in general is the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\delta^{(N-1)} = w^{(N)} \\cdot \\delta^{(N)} \\odot \\sigma^{'}_{h^{N-1}}\n",
    "\\label{ErrorTermN_1_general}\n",
    "\\end{equation}\n",
    "where $\\odot$ is the handamard or elementwise product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep going back (to the left) in the neural network to investigate how the change in $\\sigma_3^{N-2}$ (i.e. $\\sigma_3^{1}$) can cause a change in $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multipleNeuronStructure2.PNG\" width=\"800\">\n",
    "<center> Figure 1.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the change of neuron in red circle causes changes of three neurons in the second layer, and then they cause changes in the two neurons the output layer, which change the cost. We also need to consider the change of cost due to the second neuron (i.e. $\\sigma_1(h_2)$). To express this in a general form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{bmatrix} \\delta_1^{(1)}\\\\\\delta_2^{(1)}\\end{bmatrix}\n",
    "= \\begin{bmatrix} w^{(2)}_{11} & w^{(2)}_{21} & w^{(2)}_{31} \\\\w^{(2)}_{12} & w^{(2)}_{22} & w^{(2)}_{32} \\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix} \\delta_1^{(2)}\\\\\\delta_2^{(2)}\\\\\\delta_3^{(2)}\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix} \\sigma^{'}(h_1^{(1)})\\\\\\sigma^{'}(h_2^{(1)})\\end{bmatrix}\n",
    "\\label{dCdh_1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have obtained the change of $C$ with respect to $h_i$ in the first layer. Now let us remember the actual goal here is to investigate how the change in weights caused the change in the cost. That's right, it is the change of weights $w$ that changes the $h_i$. This connects with the expression in equation \\ref{dCdh_1}: $\\frac{\\delta C}{\\delta h_i^{1}}$, which leads to the change of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multipleNeuronStructure4.PNG\" width=\"800\">\n",
    "<center> Figure 1.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, since we know that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^{(1)}_1=w_{11}*x_1 + w_{12}*x_2 + w_{13}*x_3\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then its derivative wrt. $w_{11}$ (i.e. $\\frac{\\delta h^{(1)}_1}{\\delta w_{11}}$) is just $x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can use the chain rule to obtain the change of $C$ due to the change of $w_{11}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta w_{11}}=\\frac{\\delta C}{\\delta h_{1}^{(1)}}\\frac{\\delta h_{1}^{(1)}}{\\delta w_{11}}=\\delta_1^{(1)}*x_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can update the weight by using the equation 20 in chapter 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having understood backpropagation in the abstract, we can now understand the code used to implement backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "    \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "    gradient for the cost function C_x.  ``nabla_b`` and\n",
    "    ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    # backward pass\n",
    "    delta = self.cost_derivative(activations[-1], y) * \\\n",
    "        sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    # Note that the variable l in the loop below is used a little\n",
    "    # differently to the notation in Chapter 2 of the book.  Here,\n",
    "    # l = 1 means the last layer of neurons, l = 2 is the\n",
    "    # second-last layer, and so on.  It's a renumbering of the\n",
    "    # scheme in the book, used here to take advantage of the fact\n",
    "    # that Python can use negative indices in lists.\n",
    "    for l in xrange(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
