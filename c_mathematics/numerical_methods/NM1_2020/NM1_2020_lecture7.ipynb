{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods\n",
    "\n",
    "# Lecture 7: Numerical Linear Algebra III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "## I.     Ill - Conditioned Matrices\n",
    "## II.    Round Off Errors\n",
    "## III.   Algorithm Stability\n",
    "## IV.   Direct vs Iterative Methods\n",
    "## V.    Iterative Methods - Jacobi's Method\n",
    "## VI.   Iterative Methods - Gauss - Seidel Method\n",
    "## VII.  Sparse Matrices\n",
    "## VIII.  EXTRA: How to Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives:\n",
    "\n",
    "* Introduce ill-conditioned matrices (via matrix norms and condition number)\n",
    "* Consider direct vs iterative/indirect methods\n",
    "* Example iterative algorithm: the Jacobi and Gauss-Seidel methods\n",
    "* Sparse matrices and a pointer to more advanced algorithms (supplementary readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII.    Sparse matrices\n",
    "\n",
    "Note that the matrices which result from the numerical solution of differential equations are generally  *sparse* (<https://en.wikipedia.org/wiki/Sparse_matrix>) which means that most entries are zero (the alternative is termed *dense*).  Knowing which entries are zero means that we can devise more efficient matrix storage methods, as well as more efficient implementations of the above algorithms (e.g. by not bothering to do operations that we know involve multiplications by zero - we know the answer will be zero).\n",
    "\n",
    "As an example, for the two iterative methods shown above (Jacobi and Gauss Seidel), the cost of *each* iteration is quadratically dependent on the number of unknowns $n$, since we need to loop through all the entries of the $n\\times n$ matrix $A$. For a fixed number of iterations the computational cost of these methods therefore scales as $n^2$. If we know that each row only contains a fixed, small number of non-zero entries however (as for example the matrix in the example below), we can simply skip the zero entries and the cost *per iteration* becomes linear in $n$. These scalings of $n^2$ for *dense* and $n$ for *sparse* matrices for the cost per iteration are typical for iterative methods. Unfortunately this does not mean that the overall cost of an iterative method is also $n^2$ or $n$, as the number of iterations that is needed to achieve a certain accuracy quite often also increases for larger problem sizes. The number of required iterations typically only increases very slowly however, so that the cost of the method is still considerably cheaper than direct methods, in particular for very large problems.\n",
    "\n",
    "A huge range of iterative solution methods exist and the literature on this topic is massive. Below is an example of using scipy to access the Conjugate Gradient algorithm which is a popular example of a method suitable for matrices which result from the numerical solution of differential equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.          0.01579799  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01579799 -2.          0.96907927 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.96907927 -2.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -2.          0.293628\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.293628   -2.\n",
      "   0.02000908]\n",
      " [ 0.          0.          0.         ...  0.          0.02000908\n",
      "  -2.        ]]\n",
      "This is the same matrix but now stored in a sparse matrix data structure.\n",
      "  (0, 0)\t-2.0\n",
      "  (0, 1)\t0.01579798887096051\n",
      "  (1, 0)\t0.01579798887096051\n",
      "  (1, 1)\t-2.0\n",
      "  (1, 2)\t0.9690792657586554\n",
      "  (2, 1)\t0.9690792657586554\n",
      "  (2, 2)\t-2.0\n",
      "  (2, 3)\t0.4426848628892317\n",
      "  (3, 2)\t0.4426848628892317\n",
      "  (3, 3)\t-2.0\n",
      "  (3, 4)\t0.33287797242853423\n",
      "  (4, 3)\t0.33287797242853423\n",
      "  (4, 4)\t-2.0\n",
      "  (4, 5)\t0.8349961890669807\n",
      "  (5, 4)\t0.8349961890669807\n",
      "  (5, 5)\t-2.0\n",
      "  (5, 6)\t0.2759243129018931\n",
      "  (6, 5)\t0.2759243129018931\n",
      "  (6, 6)\t-2.0\n",
      "  (6, 7)\t0.5899642313423469\n",
      "  (7, 6)\t0.5899642313423469\n",
      "  (7, 7)\t-2.0\n",
      "  (7, 8)\t0.5338100213881314\n",
      "  (8, 7)\t0.5338100213881314\n",
      "  (8, 8)\t-2.0\n",
      "  :\t:\n",
      "  (41, 41)\t-2.0\n",
      "  (41, 42)\t0.6990321498971347\n",
      "  (42, 41)\t0.6990321498971347\n",
      "  (42, 42)\t-2.0\n",
      "  (42, 43)\t0.44120676550363935\n",
      "  (43, 42)\t0.44120676550363935\n",
      "  (43, 43)\t-2.0\n",
      "  (43, 44)\t0.6627012118552\n",
      "  (44, 43)\t0.6627012118552\n",
      "  (44, 44)\t-2.0\n",
      "  (44, 45)\t0.08354089874916015\n",
      "  (45, 44)\t0.08354089874916015\n",
      "  (45, 45)\t-2.0\n",
      "  (45, 46)\t0.06076481090638697\n",
      "  (46, 45)\t0.06076481090638697\n",
      "  (46, 46)\t-2.0\n",
      "  (46, 47)\t0.2600948682739955\n",
      "  (47, 46)\t0.2600948682739955\n",
      "  (47, 47)\t-2.0\n",
      "  (47, 48)\t0.29362800321791604\n",
      "  (48, 47)\t0.29362800321791604\n",
      "  (48, 48)\t-2.0\n",
      "  (48, 49)\t0.02000907848336786\n",
      "  (49, 48)\t0.02000907848336786\n",
      "  (49, 49)\t-2.0\n",
      "Now we execute the CG algorithm on our problem, with our callback function returning information on the residual at each iteration.\n",
      "1 2.3562251696335856\n",
      "2 1.0239182056504734\n",
      "3 0.401992541842747\n",
      "4 0.223770573282838\n",
      "5 0.10784772944319773\n",
      "6 0.04528275512406776\n",
      "7 0.016783187792529596\n",
      "8 0.007841550759271124\n",
      "9 0.003427366089593852\n",
      "10 0.0013261858767582826\n",
      "11 0.0005783870772007964\n",
      "12 0.0002434899030345161\n",
      "13 7.049175749160568e-05\n",
      "14 2.7297491399353727e-05\n",
      "15 9.758147321408194e-06\n",
      "16 2.1383778140400358e-06\n",
      "17 8.453340360899756e-07\n",
      "18 4.027234283292924e-07\n",
      "19 1.244334213076751e-07\n",
      "20 3.765897211891967e-08\n",
      "21 1.380157208085994e-08\n",
      "22 3.2245429787673083e-09\n",
      "23 1.211810534395043e-09\n",
      "24 3.697853422438397e-10\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse.linalg\n",
    "\n",
    "n = 50\n",
    "main_diag = np.ones(n)   #just a vector of ones\n",
    "off_diag = np.random.random(n-1)  # to make it a bit more interesting make the off-diagonals random\n",
    "A = np.diag(-2*main_diag,0) + np.diag(1.*off_diag,1) + np.diag(1.*off_diag,-1)\n",
    "# A random RHS vector\n",
    "b = np.random.random(A.shape[0])\n",
    "\n",
    "print(A) # print our A in \"dense\" matrix format\n",
    "\n",
    "sA = scipy.sparse.csr_matrix(A) # The same matrix in a \"sparse\" matrix data structure where only non-zeros stored\n",
    "print('This is the same matrix but now stored in a sparse matrix data structure.')\n",
    "print(sA)\n",
    "\n",
    "# now use a scipy iterative algorithm (Conjugate Gradient) to solve\n",
    "\n",
    "# First define a (callback) function which we are allowed to pass to the solver; here this is coded such that it will store and print the iteration numbers and residuals - basically a method to output some diagnostic information on the solver as it executes\n",
    "def gen_callback_cg():\n",
    "    diagnostics = dict(it=0, residuals=[]) \n",
    "    def callback(xk):   # xk is the solution computed by CG at each iteration\n",
    "        diagnostics[\"it\"] += 1\n",
    "        diagnostics[\"residuals\"].append(sl.norm(A @ xk - b))\n",
    "        print(diagnostics[\"it\"], sl.norm(A @ xk - b))\n",
    "    return callback    \n",
    "\n",
    "print('Now we execute the CG algorithm on our problem, with our callback function returning information on the residual at each iteration.')\n",
    "x_sol = scipy.sparse.linalg.cg(A,b,x0=None, tol=1e-10, maxiter=1000, callback=gen_callback_cg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## An example of a sparse matrix\n",
    "\n",
    "Let us consider an electric circuit arranged in a regular grid of $n$ rows and $m$ columns. The nodes in the grid are numbered from 0 to $nm-1$ as indicated in the diagram below.\n",
    "\n",
    "![bla](images/circuit.png)\n",
    "\n",
    "We want to calculate the electric potential $V_i$ in all of the nodes $i$. A node $i$ somewhere in the middle of the circuit is connected via resistor to nodes $i-1$ and $i+1$ to the left and right respectively, and to the nodes $i-m$ and $i+m$ in the rows above and below. For simplicity we assume that all resistors have the same resistance value $R$. The first and last node of the circuit (0 and $nm-1$) to a battery via two additional resistors, with the same resistance value $R$.\n",
    "\n",
    "The sum of the currents coming into a node is zero (if we use a sign convention where a current coming into a node is positive and a current going out is negative. The currents between two nodes can be calculated using Ohm's law: $I=V/R$ where $R$ is the resistance of the resistor, and $V$ is the potential difference between two nodes, say $V=V_i-V_{i-1}$. Therefore we can write:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  0 &=& I_{i-1\\to i} + I_{i+1\\to i} + I_{i-m\\to i} + I_{i+m\\to i} \\\\\n",
    "    &=& V_{i-1\\to i}/R + V_{i+1\\to i}/R + V_{i-m\\to i}/R + V_{i+m\\to i}/R \\\\\n",
    "    &=& (V_{i}-V_{i-1})/R + (V_{i}-V_{i+1})/R + (V_{i}-V_{i-m})/R + (V_{i}-V_{i+m})/R \\\\\n",
    "    &=& (4V_{i}-V_{i-1}-V_{i+1}-V_{i-m}-V_{i+m})/R\n",
    "\\end{eqnarray}\n",
    "\n",
    "This gives us one linear equation for each node in the circuit (with slight modifications for nodes that are not in the interior). These can be combined into a linear system $Ax=b$ which is assembled in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. -1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  3. -1.  0. -1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  2.  0.  0. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  3. -1.  0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0. -1.  4. -1.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0. -1.  3.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  3. -1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0. -1.  4. -1.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0. -1.  0. -1.  3.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1.  0.  0.  2. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0. -1.  3. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0. -1.  3.]]\n"
     ]
    }
   ],
   "source": [
    "n = 4 # number of rows\n",
    "m = 3 # number of columns\n",
    "V_battery = 5.0 # voltage on the right of the battery\n",
    "\n",
    "A = np.zeros((n*m, n*m))\n",
    "for row in range(n):\n",
    "    for column in range(m):\n",
    "        i = row*m + column # node number\n",
    "        if column>0: # left neighbour\n",
    "            A[i,i-1] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "        if column<m-1: # right neighbour\n",
    "            A[i,i+1] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "        if row>0: # neighbour above\n",
    "            A[i,i-m] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "        if row<n-1: # neighbour below\n",
    "            A[i,i+m] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "\n",
    "# connecting node 0 to the battery: I = (V_0 - 0)/R\n",
    "A[0,0] += 1.0 \n",
    "# connecting last node nm-1 to the battery: I = (V_0 - V_battery)/R = V_0/R - V_battery/R\n",
    "A[n*m-1,n*m-1] += 1.0\n",
    "# the V_battery/R term is a constant that does not depend on the unknowns, so ends up in the rhs vector b\n",
    "b = np.zeros(n*m)\n",
    "b[n*m-1] = V_battery\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA.    How - to - Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always check the solvability before actually trying to solve it! You could use determinant and Norms, or the Gaussian - Jordan to check!\n",
    "\n",
    "For system of linear equations\n",
    "\n",
    "- If the problem you are solving has very few unknowns and very few equations, i.e. 3 or less, maybe more for some of you who are more math savvy, then solve it by hand\n",
    "\n",
    "- If the problem you are solving has few unknowns and few equations, use Gaussian Elimination, preferably through LU decomposition using Doolittle Algorithm and maybe implement partial pivoting for $0$ on the diagonal\n",
    "\n",
    "- f the problem you are solving has lots of unknowns and lots of equation, and finding the exact solution would take ages, and well you don't have ages of time to solve the problem, then preferably Iterative Methods, such as the Jacobi Method or Gauss - Seidel Method. \n",
    "\n",
    "For numerical solutions to differential equations that are generally sparse - maybe use Conjugate Gradient Algorithm or some other algorithm that reduces the number of operations for sparse matrices, especially for very large sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213.809px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
