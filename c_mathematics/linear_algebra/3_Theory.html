

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Theory &#8212; ESE Jupyter Material</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".cell"
        const thebe_selector_input = ".cell_input div.highlight"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Eigenvalues and eigenvectors" href="4_Eigenvalues_and_eigenvectors.html" />
    <link rel="prev" title="Systems of linear equations" href="2_Linear_Systems.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ESE Jupyter Material</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../intro.html">Landing page</a>
  </li>
  <li class="">
    <a href="../../a_modules/intro.html">Modules</a>
  </li>
  <li class="">
    <a href="../../b_coding/intro.html">Coding</a>
  </li>
  <li class="active">
    <a href="../intro.html">Mathematics</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="../calculus/intro.html">Calculus</a>
    </li>
    <li class="">
      <a href="../complex_analysis/intro.html">Complex analysis</a>
    </li>
    <li class="">
      <a href="../differential_equations/intro.html">Differential equations</a>
    </li>
    <li class="active">
      <a href="intro.html">Linear algebra</a>
      <ul class="nav sidenav_l3">
      <li class="">
        <a href="1_Basic_definitions_and_operations.html">Basic definitions and operations</a>
      </li>
      <li class="">
        <a href="2_Linear_Systems.html">Systems of linear equations</a>
      </li>
      <li class="active">
        <a href="">Theory</a>
      </li>
      <li class="">
        <a href="4_Eigenvalues_and_eigenvectors.html">Eigenvalues and eigenvectors</a>
      </li>
      <li class="">
        <a href="5_Linear_Algebra_in_Python.html">Linear Algebra in Python</a>
      </li>
    </ul>
    </li>
    <li class="">
      <a href="../mathematical_notation/intro.html">Mathematical notation</a>
    </li>
    <li class="">
      <a href="../numerical_methods/intro.html">Numerical methods</a>
    </li>
    <li class="">
      <a href="../series_and_sequences/intro.html">Series and sequences</a>
    </li>
    <li class="">
      <a href="../sets_and_functions/intro.html">Sets and functions</a>
    </li>
    <li class="">
      <a href="../statistics/intro.html">Statistics</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../../d_geosciences/intro.html">Geosciences</a>
  </li>
  <li class="">
    <a href="../../e_extra/intro.html">Further resources</a>
  </li>
  <li class="">
    <a href="../../genindex.html">Index</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/c_mathematics/linear_algebra/3_Theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/primer-computational-mathematics/book/issues/new?title=Issue%20on%20page%20%2Fc_mathematics/linear_algebra/3_Theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/primer-computational-mathematics/book/master?urlpath=tree/notebooks/c_mathematics/linear_algebra/3_Theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#vector-spaces" class="nav-link">Vector spaces</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#linear-combination-of-vectors" class="nav-link">Linear combination of vectors</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h4">
            <a href="#example" class="nav-link">Example</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#spanning-set-basis" class="nav-link">Spanning set & Basis</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h4">
            <a href="#example-minimal-spanning-set" class="nav-link">Example: Minimal spanning set</a>
        </li>
    
        <li class="nav-item toc-entry toc-h4">
            <a href="#example-basis" class="nav-link">Example: Basis</a>
        </li>
    
        <li class="nav-item toc-entry toc-h4">
            <a href="#example-mathbb-r-n-vector-space" class="nav-link">Example: \mathbb{R}^n vector space</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#change-of-basis" class="nav-link">Change of basis</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#range" class="nav-link">Range</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#rank" class="nav-link">Rank</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#null-space" class="nav-link">Null space</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#example-1" class="nav-link">Example 1</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#example-2" class="nav-link">Example 2</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#inverse-of-a-matrix" class="nav-link">Inverse of a matrix</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#invertible-matrix-theorem" class="nav-link">Invertible Matrix Theorem</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h4">
            <a href="#example-rotation-matrix" class="nav-link">Example: Rotation matrix</a>
        </li>
    
            </ul>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#determinant" class="nav-link">Determinant</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#adjugate-matrix" class="nav-link">Adjugate matrix</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#example-inverse-of-a-2-times-2-matrix" class="nav-link">Example: Inverse of a 2 \times 2 matrix</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#cramer-s-rule" class="nav-link">Cramer’s rule</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#example-solving-a-system-of-complex-linear-equations" class="nav-link">Example: Solving a system of complex linear equations</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#vector-norms" class="nav-link">Vector norms</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#examples" class="nav-link">Examples</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#matrix-norms" class="nav-link">Matrix norms</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="theory">
<span id="linalg-theory"></span><h1>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h1>
<div class="section" id="vector-spaces">
<h2>Vector spaces<a class="headerlink" href="#vector-spaces" title="Permalink to this headline">¶</a></h2>
<p id="index-0">An n-dimensional vector space <span class="math notranslate nohighlight">\(V^n\)</span> is a set of <em>all</em> n-tuples (sequences of <span class="math notranslate nohighlight">\(n\)</span> scalars) <span class="math notranslate nohighlight">\((x_1, x_2, \dots, x_n)\)</span> which we call points or vectors, such that <span class="math notranslate nohighlight">\(V^n\)</span> is closed under addition and scalar multiplication and these operations satisfy the following properties:</p>
<ol class="simple">
<li><p>Associativity of addition: <span class="math notranslate nohighlight">\(\mathbf{x} + (\mathbf{y} + \mathbf{z}) = (\mathbf{x} + \mathbf{y}) + \mathbf{z}\)</span></p></li>
<li><p>Commutativity of addition: <span class="math notranslate nohighlight">\(\mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}\)</span></p></li>
<li><p>There exists an element <span class="math notranslate nohighlight">\(\mathbf{0} \in V^n\)</span>, called the zero vector, such that <span class="math notranslate nohighlight">\(\mathbf{x} + \mathbf{0} = \mathbf{x}\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in V^n\)</span></p></li>
<li><p>For all <span class="math notranslate nohighlight">\(\mathbf{x} \in V^n\)</span> there exists an inverse element <span class="math notranslate nohighlight">\(-\mathbf{x} \in V^n\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{x} + (-\mathbf{x}) = \mathbf{0}\)</span></p></li>
<li><p>Distributivity of scalar multiplication with respect to addition in <span class="math notranslate nohighlight">\(V^n\)</span>: <span class="math notranslate nohighlight">\(\alpha (\mathbf{x} + \mathbf{y}) = \alpha \mathbf{x} + \alpha \mathbf{y}\)</span> for all <span class="math notranslate nohighlight">\(x, y \in V^n\)</span> and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span></p></li>
<li><p>Distributivity of scalar multiplication with respect to addition in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>: \( (\alpha + \beta)\mathbf{x} = \alpha \mathbf{x} + \beta \mathbf{x} \)</p></li>
<li><p>Compatibility of multiplication: <span class="math notranslate nohighlight">\(\alpha (\beta \mathbf{x}) = (\alpha \beta)\mathbf{x}\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in V^n\)</span> and <span class="math notranslate nohighlight">\(\alpha, \beta \in \mathbb{R}\)</span></p></li>
<li><p>Identity element of scalar multiplication: <span class="math notranslate nohighlight">\(1\mathbf{x} = \mathbf{x}\)</span>, where <span class="math notranslate nohighlight">\(1\)</span> denotes the multiplicative identity in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></li>
</ol>
<p>For a set to be closed under addition, it means that \( \mathbf{x} + \mathbf{y} \in V^n\) for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V^n\)</span>. Or in words, a sum of two arbitrary vectors from <span class="math notranslate nohighlight">\(V^n\)</span> is also in <span class="math notranslate nohighlight">\(V^n\)</span>. Similarly, for the set to be closed under scalar multiplication, <span class="math notranslate nohighlight">\(\alpha \mathbf{x} \in V^n\)</span> for all <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} \in V^n\)</span>. These two operations allow us to freely scale our vectors and to form linear combinations of vectors while still remaining in the same vector space.</p>
<div class="section" id="linear-combination-of-vectors">
<h3>Linear combination of vectors<a class="headerlink" href="#linear-combination-of-vectors" title="Permalink to this headline">¶</a></h3>
<p id="index-1">Let <span class="math notranslate nohighlight">\(\mathbf{z} \in V^n\)</span> and let \( \mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_k\) also be vectors in <span class="math notranslate nohighlight">\(V^n\)</span>. If we can write <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{z} = \alpha_1 \mathbf{u}_1 + \alpha_2 \mathbf{u}_2 + \dots + \alpha_k \mathbf{u}_k \]</div>
<p>for some scalars <span class="math notranslate nohighlight">\(\alpha_1, \alpha_2, \dots, \alpha_k\)</span>, then we say that <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is a <em>linear combination</em> of a set of vectors <span class="math notranslate nohighlight">\(\{ \mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_k \}\)</span>.</p>
<p>We say that a set of vectors <span class="math notranslate nohighlight">\(\{ \mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n \}\)</span> is <strong>linearly independent</strong> iff vector <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span>, <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>, <em>cannot</em> be written as a linear combination of the other vectors. More generally, a set of vectors <span class="math notranslate nohighlight">\(\{ \mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n \}\)</span> is linearly independent if</p>
<div class="math notranslate nohighlight">
\[ \alpha_1 \mathbf{u}_1 + \alpha_2 \mathbf{u}_2 + \cdots + \alpha_n \mathbf{u}_n = \mathbf{0}\]</div>
<p>is possible only for <span class="math notranslate nohighlight">\(\alpha_i = 0\)</span>.</p>
<div class="section" id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>Consider the following vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 2 \\ 2 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} -1 \\ 3 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 4 \\ -1 \\ 0 \end{bmatrix}.\end{split}\]</div>
<p>Notice that the last 3 vectors can be written as a linear combination of the first 2 vectors. For example,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} -1 \\ 3 \\ 0 \end{bmatrix} =
-1 \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + 3
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}.\end{split}\]</div>
</div>
</div>
<div class="section" id="spanning-set-basis">
<h3>Spanning set &amp; Basis<a class="headerlink" href="#spanning-set-basis" title="Permalink to this headline">¶</a></h3>
<span class="target" id="index-2"></span><p id="index-3">Consider the following vectors in <span class="math notranslate nohighlight">\(V^n\)</span> whose coordinates are all 0 except for one which equals 1:</p>
<div class="math notranslate nohighlight">
\[\begin{split} e_1 = (1, 0, 0, \dots, 0) \\
e_2 = (0, 1, 0, \dots, 0) \\
e_3 = (0, 0, 1, \dots, 0) \\
\vdots \\ e_n = (0, 0, 0, \dots, 1). \end{split}\]</div>
<p>Every vector <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \dots, x_n) \in V^n\)</span> can be represented as:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{x} = x_1 e_1 + x_2 e_2 + \dots + x_n e_n. \]</div>
<p>In other words, every vector <span class="math notranslate nohighlight">\(\mathbf{x} \in V^n\)</span> can be represented as a linear combination of a set of vectors <span class="math notranslate nohighlight">\(S = \{ e_1, \dots, e_n \}\)</span>. We say that <span class="math notranslate nohighlight">\(S\)</span> is a <strong>spanning set</strong> of <span class="math notranslate nohighlight">\(V^n\)</span>. Equivalently, we can say that <span class="math notranslate nohighlight">\(V^n\)</span> is <em>spanned</em> by a set of vectors <span class="math notranslate nohighlight">\(S\)</span>.</p>
<p>Furthermore, if <span class="math notranslate nohighlight">\(S\)</span> is linearly independent it is also a <em>minimal spanning set</em> of <span class="math notranslate nohighlight">\(V^n\)</span> (i.e. there is no subset of <span class="math notranslate nohighlight">\(S\)</span> that spans <span class="math notranslate nohighlight">\(V^n\)</span>). We then call it a <strong>basis</strong> of <span class="math notranslate nohighlight">\(V^n\)</span>. Unlike in spanning sets, the order of vectors is important in a basis. So a basis is a <em>sequence</em> of <span class="math notranslate nohighlight">\(n\)</span> linearly independent vectors that span a vector space <span class="math notranslate nohighlight">\(V^n\)</span>, such that every vector <span class="math notranslate nohighlight">\(\mathbf{x} \in V^n\)</span> is a <em>unique</em> linear combination of vectors of the basis.</p>
<p>In summary, in <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector space <span class="math notranslate nohighlight">\(V^n\)</span> the following three statements are equivalent:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\((u_1, u_2, \dots, u_n)\)</span> is a basis of <span class="math notranslate nohighlight">\(V^n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\{ u_1, u_2, \dots, u_n \}\)</span> is a linearly independent set</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{span}(\{u_1, u_2, \dots, u_n\}) = V^n.\)</span></p></li>
</ol>
<div class="section" id="example-minimal-spanning-set">
<h4>Example: Minimal spanning set<a class="headerlink" href="#example-minimal-spanning-set" title="Permalink to this headline">¶</a></h4>
<p>Let a subspace <span class="math notranslate nohighlight">\(X \subset \mathbb{R}^3\)</span> be spanned by vectors</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 2 \\ 2 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} -1 \\ 3 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 4 \\ -1 \\ 0 \end{bmatrix},\end{split}\]</div>
<p>and let <span class="math notranslate nohighlight">\(Y \subset \mathbb{R}^3\)</span> be spanned by vectors</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}.\end{split}\]</div>
<p>Notice that vectors</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix} 2 \\ 2 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} -1 \\ 3 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 4 \\ -1 \\ 0 \end{bmatrix}\end{split}\]</div>
<p>can be written as a linear combination of the other 2 vectors. They are therefore redundant in the definition of a subspace <span class="math notranslate nohighlight">\(X\)</span>. Therefore, the only vectors which form a linearly independent set are</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}.\end{split}\]</div>
<p>We can then conclude that <span class="math notranslate nohighlight">\(X = Y\)</span> and they include all vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> whose third coordinate is equal to 0.</p>
</div>
<div class="section" id="example-basis">
<h4>Example: Basis<a class="headerlink" href="#example-basis" title="Permalink to this headline">¶</a></h4>
<p>Let us show that vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split} a_1 = \begin{pmatrix} 1 \\ 1 \\ -1 \end{pmatrix}, \quad
a_2 = \begin{pmatrix} -1 \\ 2 \\ 1 \end{pmatrix}, \quad
a_3 = \begin{pmatrix} 2 \\ -1 \\ 1 \end{pmatrix} \quad \end{split}\]</div>
<p>form a basis for <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. For these vectors to a basis, they need to form a linearly independent set in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. We will test that by Gaussian eliminations on the <span class="math notranslate nohighlight">\(3 \times 3\)</span> matrix <span class="math notranslate nohighlight">\((a_1, a_2, a_3)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{pmatrix} 1 &amp; -1 &amp; 2 \\ 1 &amp; 2 &amp; -1 \\ -1 &amp; 1 &amp; 1 \end{pmatrix} \Rightarrow 
\begin{pmatrix} 1 &amp; -1 &amp; 2 \\ 0 &amp; 3 &amp; -3 \\ -1 &amp; 1 &amp; 1 \end{pmatrix} \Rightarrow
\begin{pmatrix} 1 &amp; -1 &amp; 2 \\ 0 &amp; 3 &amp; -3 \\ 0 &amp; 0 &amp; 3 \end{pmatrix}\end{split}\]</div>
<p>From the last matrix we see that the three columns are linearly independent, so <span class="math notranslate nohighlight">\((a_1, a_2, a_3)\)</span> indeed do form a basis for <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. If we wanted, we could continue to reduce the matrix to a reduced row echelon form (RREF) to get:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{split}\]</div>
<p>From here it is completely obvious that the columns are linearly independent.</p>
<p>We could have approached this problem another way. We could have formulated it by saying that we wanted to prove that for any RHS <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^3\)</span> the system of equations</p>
<div class="math notranslate nohighlight">
\[ \lambda_1 a_1 + \lambda_2 a_2 + \lambda_3 a_3 = \mathbf{x} \]</div>
<p>has a unique solution, where <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2, \lambda_3\)</span>. The process is still be the same, as the solution of a system of linear equation is unique only if all columns are linearly independent.</p>
</div>
<div class="section" id="example-mathbb-r-n-vector-space">
<h4>Example: <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> vector space<a class="headerlink" href="#example-mathbb-r-n-vector-space" title="Permalink to this headline">¶</a></h4>
<p>We can envisage sets <span class="math notranslate nohighlight">\(\mathbb{R}^1, \mathbb{R}^2\)</span> and <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> geometrically. For example, we think of triplets <span class="math notranslate nohighlight">\((x_1, x_2, x_3)\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> as points in the entire 3-D coordinate space with coordinates <span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span>. The standard basis of <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> is a set of vectors <span class="math notranslate nohighlight">\(\{ e_1, e_2, e_3 \}\)</span> which we might denote as unit vectors <span class="math notranslate nohighlight">\(\hat{x}, \hat{y}, \hat{z}\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(n &gt; 3\)</span> we do not have a geometrical image of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. For example, we can represent wind velocity <span class="math notranslate nohighlight">\((v_1, v_2, v_3)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> in space with coordinates <span class="math notranslate nohighlight">\((x, y, z)\)</span> as a point <span class="math notranslate nohighlight">\((v_1, v_2, v_3, t, x, y, z)\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^7\)</span>.</p>
</div>
</div>
<div class="section" id="change-of-basis">
<h3>Change of basis<a class="headerlink" href="#change-of-basis" title="Permalink to this headline">¶</a></h3>
<p>Let us go back to a matrix-vector multiplication we considered at the end of the first notebook. Some may remember this figure:</p>
<img src="linalgdata/chofbasis.png" width="400">
<p>We considered a general matrix left-multiplying the basis vectors. Let us recall what we found:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} a \\ c \end{pmatrix}, \qquad 
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} b \\ d \end{pmatrix}.\end{split}\]</div>
<p>As suggested by the title, we can therefore consider matrix multiplication as a change of basis. Say we are multiplying <span class="math notranslate nohighlight">\(A \mathbf{u} = \mathbf{v}\)</span>. What this means is that the vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is the vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> transformed in the basis formed by the columns of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div>
</div>
<div class="section" id="range">
<h2>Range<a class="headerlink" href="#range" title="Permalink to this headline">¶</a></h2>
<p><strong>Range</strong> (or image) of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\text{range}(A)\)</span>, is the set of vectors that can be expressed as <span class="math notranslate nohighlight">\(A \mathbf{x}\)</span> for some <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Recall that <span class="math notranslate nohighlight">\(A \mathbf{x}\)</span> can be expressed as a linear combination of columns of <span class="math notranslate nohighlight">\(A\)</span>, so the range is also called the <strong>column space</strong>. Therefore, it is the <em>span</em> (set of all possible linear combinations) of its column vectors.</p>
<p>For example, if <span class="math notranslate nohighlight">\(\text{range}(A) = \mathbb{R}^3\)</span>, that means that every point in 3-space can be reached and can therefore be written as <span class="math notranslate nohighlight">\(A \mathbf{x}\)</span>.</p>
</div>
<div class="section" id="rank">
<h2>Rank<a class="headerlink" href="#rank" title="Permalink to this headline">¶</a></h2>
<p><strong>Rank</strong> of matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\text{rank}(A)\)</span>, is the dimension of the column space of <span class="math notranslate nohighlight">\(A\)</span>. It is equal to the number of linearly independent columns (or rows) of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>An <span class="math notranslate nohighlight">\(m \times n\)</span> matrix is <em>full rank</em> iff <span class="math notranslate nohighlight">\(\text{rank}(A) = \min(m, n)\)</span>, i.e. it has maximum possible rank. A matrix that does not have full rank is <em>rank deficient</em>.</p>
</div>
<div class="section" id="null-space">
<h2>Null space<a class="headerlink" href="#null-space" title="Permalink to this headline">¶</a></h2>
<p><strong>Null space</strong> of <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(\text{null}(A)\)</span>, is the set of vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> that are a solution of a homogeneous system of linear equations <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span>. Null space is a subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> of dimension <span class="math notranslate nohighlight">\(n - \text{rank}(A)\)</span>. Finding the null space of a matrix will therefore involve finding the rank of a matrix, which we obtain by performing Gaussian eliminations.</p>
<p>If we find a set of <span class="math notranslate nohighlight">\(n - \text{rank}(A)\)</span> linearly independent vectors in <span class="math notranslate nohighlight">\(\text{null}(A)\)</span> they will form a basis for <span class="math notranslate nohighlight">\(\text{null}(A)\)</span>. We can then express every vector in <span class="math notranslate nohighlight">\(\text{null}(A)\)</span> as a linear combination of the basis vectors.</p>
<p><strong>Side note.</strong> A non-homogeneous system of linear equations <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span> can be equivalently written as \( A\mathbf{x} + A\mathbf{x}_h = \mathbf{b} + \mathbf{0}\), where <span class="math notranslate nohighlight">\(A\mathbf{x}_h = \mathbf{0}\)</span> is a homogeneous system with solution <span class="math notranslate nohighlight">\(\mathbf{x}_h \in \text{null}(A)\)</span>, since <span class="math notranslate nohighlight">\(\mathbf{b} + \mathbf{0} = \mathbf{b}\)</span>. We can further simplify this by writing:</p>
<div class="math notranslate nohighlight">
\[A\mathbf{x} + A\mathbf{x}_h = A(\mathbf{x} + \mathbf{x}_h) = \mathbf{b}. \]</div>
<p>This means that to any solution <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of a non-homogeneous system we can add any vector <span class="math notranslate nohighlight">\(\mathbf{x}_h\)</span> from the null space and get another solution of the system.</p>
<div class="section" id="example-1">
<h3>Example 1<a class="headerlink" href="#example-1" title="Permalink to this headline">¶</a></h3>
<p>Consider a homogeneous system of linear equations <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span> where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{4 \times 3}\)</span> is the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split} A = \begin{bmatrix}
1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \\ 10 &amp; 11 &amp; 15
\end{bmatrix} \overset{RREF}{\longrightarrow} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0
\end{bmatrix} = A_R, \end{split}\]</div>
<p>where we have reduced <span class="math notranslate nohighlight">\(A\)</span> to its RREF <span class="math notranslate nohighlight">\(A_R\)</span>. We know that the systems of equations with coefficient matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A_R\)</span> are equivalent, which means that they have the same set of solutions. This set of solutions is the null space. Here <span class="math notranslate nohighlight">\(A\)</span> is full rank, i.e. <span class="math notranslate nohighlight">\(\text{rank}(A) = 3\)</span>. That means that the homogeneous system of equations above has a unique solution. Since <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> is always a solution, we conclude that this unique solution must be the null-vector <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>. In other words, the solution set of <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{0}\)</span> is the set <span class="math notranslate nohighlight">\(\text{null}(A) = \text{null}(A_R)\)</span> = { \mathbf{0} }.</p>
</div>
<div class="section" id="example-2">
<h3>Example 2<a class="headerlink" href="#example-2" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be <span class="math notranslate nohighlight">\(\in \mathbb{R}^{5 \times 8}\)</span> with its RREF <span class="math notranslate nohighlight">\(A_R\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} A_R = \begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 3 &amp; 0  &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1  &amp; -2 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix} \end{split}\]</div>
<p><span class="math notranslate nohighlight">\(A_R\)</span> has 4 non-zero rows, so <span class="math notranslate nohighlight">\(\text{rank}(A) = 4\)</span>. Since <span class="math notranslate nohighlight">\(n=8\)</span> the system has infinitely many solutions and we know that <span class="math notranslate nohighlight">\(\text{dim}(\text{null}(A)) = n - \text{rank}(A) = 8 - 4 = 4\)</span>.</p>
<p>There are several ways we could find the null space. For computation by Gaussian elimination, the reader is encouraged to go through <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_(linear_algebra)#Computation_by_Gaussian_elimination">this example</a>. Another method, which we show below, is to explicitly write the coordinates <span class="math notranslate nohighlight">\(x_i\)</span> of the solution <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> for <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span>. The augmented matrix with <span class="math notranslate nohighlight">\(A_R\)</span> on the left part and <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> on the right is in this case:</p>
<div class="math notranslate nohighlight">
\[\begin{split} [A_R | \mathbf{0}] = \left [
\begin{array}{cccccccc|c}
1 &amp; 2 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 3 &amp; 0  &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1  &amp; -2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{array} \right ] \end{split}\]</div>
<p>From here we explicitly write the equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split} x_1 + 2x_2 + x_4 - x_5 + 3x_7 = 0 \\
x_3 + x_4 + 3x_5 + x_7 = 0 \\
x_6 - 2x_7 = 0 \\
x_8 = 0\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(x_2, x_4, x_5, x_7\)</span> are free variables (notice that their indices match the index of their non-pivotal column, sometimes called free column), so we can parameterise above equations such that <span class="math notranslate nohighlight">\(x_2 = \alpha\)</span>, <span class="math notranslate nohighlight">\(x_4 = \beta\)</span>, <span class="math notranslate nohighlight">\(x_5 = \gamma\)</span>, <span class="math notranslate nohighlight">\(x_7 = \delta\)</span>, where <span class="math notranslate nohighlight">\(\alpha, \beta, \gamma, \delta \in \mathbb{R}\)</span>. We get:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split} x_1 = -2\alpha - \beta + \gamma - 3\delta \\
x_3 = -\beta -3\gamma -\delta \\
x_6 = 2\delta$$.\end{split}\\The solution $\mathbf{x}$ is therefore:\\\begin{split}$$ \mathbf{x} = \begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8 \end{bmatrix} = \begin{bmatrix} -2\alpha - \beta + \gamma - 3\delta \\ \alpha \\ -\beta -3\gamma -\delta \\ \beta \\ \gamma \\ 2\delta \\ \delta \\ 0 \end{bmatrix}.\end{split}\end{aligned}\end{align} \]</div>
<p>We can rewrite this as a linear combination of four vectors which form a basis of <span class="math notranslate nohighlight">\(\text{null}(A)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} = \alpha \begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \beta \begin{bmatrix} -1 \\ 0 \\ -1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \gamma \begin{bmatrix} 1 \\ 0 \\ -3 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \delta \begin{bmatrix} -3 \\ 0 \\ -1 \\ 0 \\ 0 \\ 2 \\ 1 \\ 0 \end{bmatrix}, \quad \alpha, \beta, \gamma, \delta \in \mathbb{R}. \end{split}\]</div>
</div>
</div>
<div class="section" id="inverse-of-a-matrix">
<h2>Inverse of a matrix<a class="headerlink" href="#inverse-of-a-matrix" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. For another square matrix <span class="math notranslate nohighlight">\(B\)</span> we say it is an <em>inverse</em> of <span class="math notranslate nohighlight">\(A\)</span> if</p>
<div class="math notranslate nohighlight">
\[ AB = BA = I. \]</div>
<p>If an inverse of <span class="math notranslate nohighlight">\(A\)</span> exists, then it is unique. Let us prove this by contradiction, by saying that there exists another matrix <span class="math notranslate nohighlight">\(B'\)</span> such that <span class="math notranslate nohighlight">\(AB' = B'A = I\)</span>. Then because of the associativity of matrix multiplication and multiplicative identity when multiplying by the identity matrix <span class="math notranslate nohighlight">\(I\)</span>:</p>
<div class="math notranslate nohighlight">
\[ B' = B'I = B'(AB) = (B'A)B = IB = B. \]</div>
<p>Not all matrices have an inverse. If the inverse of <span class="math notranslate nohighlight">\(A\)</span> exists, we denote it as <span class="math notranslate nohighlight">\(A^{-1}\)</span>.</p>
<p><strong>Properties of inverses.</strong> For invertible matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{C}^{n \times n}\)</span> and a scalar <span class="math notranslate nohighlight">\(\alpha \in \mathbb{C}\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha A\)</span> is invertible and <span class="math notranslate nohighlight">\((\alpha A)^{-1} = \alpha^{-1} A^{-1}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(AB\)</span> is invertible and <span class="math notranslate nohighlight">\((AB)^{-1} = B^{-1}A^{-1}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A^T\)</span> is invertible and <span class="math notranslate nohighlight">\((A^T)^{-1} = (A^{-1})^T\)</span></p></li>
</ol>
<div class="section" id="invertible-matrix-theorem">
<h3>Invertible Matrix Theorem<a class="headerlink" href="#invertible-matrix-theorem" title="Permalink to this headline">¶</a></h3>
<p>The invertible matrix theorem generates a series of equivalent conditions for <span class="math notranslate nohighlight">\(A \in \mathbb{C}^{n \times n}\)</span> to be invertible. Here we will name a few, all of which we will have covered in one of these notebooks.</p>
<p>For <span class="math notranslate nohighlight">\(A\)</span> to be invertible, any (and hence all) of the following equivalent conditions must hold:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> has an inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{rank}(A) = n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{range}(A) = \mathbb{C}^n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{null}(A) = \{\mathbf{0}\}\)</span>; i.e. <span class="math notranslate nohighlight">\(A\mathbf{x} = 0\)</span> iff <span class="math notranslate nohighlight">\(\mathbf{x} =\mathbf{0}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(0\)</span> is not an eigenvalue of <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(0\)</span> is not a singular value of <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\det (A) \neq 0\)</span></p></li>
</ol>
<div class="section" id="example-rotation-matrix">
<h4>Example: Rotation matrix<a class="headerlink" href="#example-rotation-matrix" title="Permalink to this headline">¶</a></h4>
<p>Consider a general 2-D rotation matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split} R = \begin{pmatrix} \cos \varphi &amp; -\sin \varphi \\ \sin \varphi &amp; \cos \varphi \end{pmatrix}. \end{split}\]</div>
<p>Let us think geometrically and try to find the inverse of this matrix. When multiplying a vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>, the product <span class="math notranslate nohighlight">\(R\mathbf{u} = \tilde{\mathbf{u}}\)</span> is a vector rotated counterclockwise in the <span class="math notranslate nohighlight">\(xy\)</span>-plane by an angle <span class="math notranslate nohighlight">\(\varphi\)</span>, measured from the positive <span class="math notranslate nohighlight">\(x\)</span> axis. An inverse of <span class="math notranslate nohighlight">\(R\)</span>, <span class="math notranslate nohighlight">\(R^{-1}\)</span>, has to be such that \( R^{-1}\tilde{\mathbf{u}} = \mathbf{u}\), i.e. it will act to rotate by exactly the same angle, but in the opposite direction. Therefore, we simply need to change the sign of <span class="math notranslate nohighlight">\(\varphi\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} R^{-1} = \begin{pmatrix} \cos (-\varphi) &amp; -\sin (-\varphi) \\ \sin (-\varphi) &amp; \cos (-\varphi) \end{pmatrix}. \end{split}\]</div>
<p>Recall that sine is an odd function (\( \sin(-\varphi) = -\sin \varphi\)), while cosine is even (<span class="math notranslate nohighlight">\( \cos (-\varphi) = \cos \varphi \)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split} R^{-1} = \begin{pmatrix} \cos \varphi &amp; \sin \varphi \\ -\sin \varphi &amp; \cos \varphi \end{pmatrix} = R^T. \end{split}\]</div>
<p>We see that the inverse <span class="math notranslate nohighlight">\(R^{-1}\)</span> coincides with the transpose <span class="math notranslate nohighlight">\(R^T\)</span>.</p>
</div>
</div>
</div>
<div class="section" id="determinant">
<h2>Determinant<a class="headerlink" href="#determinant" title="Permalink to this headline">¶</a></h2>
<p>A determinant of a square matrix is an important notion in linear algebra. Even though today determinants are exclusively used in matrix theory, they chronologically predate matrices, as they have multiple important properties. Some of them we will consider here.</p>
<p>The determinant of a matrix <span class="math notranslate nohighlight">\(A\)</span> is denoted as <span class="math notranslate nohighlight">\(\det(A), \det A\)</span> or <span class="math notranslate nohighlight">\(|A|\)</span>. Let us introduce the determinant by looking at smaller matrices, working our way up.</p>
<p><strong>1.</strong> The determinant of a <span class="math notranslate nohighlight">\(1 \times 1\)</span> matrix <span class="math notranslate nohighlight">\(a_{11}\)</span> is equal to that number <span class="math notranslate nohighlight">\(a_{11}\)</span>.
<strong>2.</strong> The determinant of a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix is the number</p>
<div class="math notranslate nohighlight">
\[\begin{split} \det \begin{pmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{pmatrix} =
a_{11}a_{22} - a_{12}a_{21},\end{split}\]</div>
<p>which is the difference between the product of the diagonal elements and the product of the off-diagonal elements. Notice that the determinant of a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix is the area of the parallelogram bounded by its column-vectors.</p>
<img src="linalgdata/parallelogram.png" width="300">
<p><strong>3.</strong> The determinant of a <span class="math notranslate nohighlight">\(3 \times 3\)</span> matrix is the number:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \left | \begin{array}{ccc} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32} &amp; a_{33} \end{array} \right | =
a_{11} \left | \begin{array}{ccc} a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{array} \right | -
a_{12} \left | \begin{array}{ccc} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{array} \right | +
a_{13} \left | \begin{array}{ccc} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{array} \right |.\end{split}\]</div>
<p>Seemingly daunting, but in reality a very simple procedure - a <span class="math notranslate nohighlight">\(3 \times 3\)</span> case of the <em>Laplace expansion</em> for determining determinants. We consider the entries <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the top row separately and use them to multiply its <em>minor</em>. A minor <span class="math notranslate nohighlight">\(M_{ij}\)</span> is the determinant of a smaller square matrix obtained by cutting out the i-th row and j-th column where the top row entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> we are multiplying it by lies. For the <span class="math notranslate nohighlight">\(3 \times 3\)</span> case, <span class="math notranslate nohighlight">\(i = 1\)</span> and <span class="math notranslate nohighlight">\(j = 1, 2, 3\)</span>. Going from left to right, we alternate between adding and subtracting these (top row entry <span class="math notranslate nohighlight">\(\times\)</span> minor) terms.</p>
<p>Laplace expansion is also called <em>cofactor expansion</em>: a <strong>cofactor</strong> <span class="math notranslate nohighlight">\(C_{ij}\)</span> is obtained by including the correct sign (alternating addition and subtraction) of the minors. That is, <span class="math notranslate nohighlight">\(C_{ij} = (-1)^{i+j}M_{ij}\)</span>.</p>
<p>Alternatively, some may find the <a class="reference external" href="https://en.wikipedia.org/wiki/Rule_of_Sarrus">Sarrus’ scheme</a> more useful in remembering the above formula.</p>
<p>A generalisation of the 2-D parallelogram, the determinant of a <span class="math notranslate nohighlight">\(3 \times 3\)</span> matrix is the volume of a parallelepiped bounded by the column vectors of the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>4.</strong> To calculate the determinant of even larger matrices, we can again use Laplace expansion.</p>
<p><strong>Properties of determinants.</strong> For square matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{C}^{n \times n}\)</span> and a scalar <span class="math notranslate nohighlight">\(\alpha \in \mathbb{C}\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(|AB| = |A| |B|\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(|A^T| = |A|\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(|A^{-1}| = \frac{1}{|A|}\)</span>. Remember that a matrix is not invertible if its determinant is 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(| \alpha A | = \alpha ^n |A|\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(|A| = 0\)</span> if <span class="math notranslate nohighlight">\(\text{rank}(A) &lt; n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(|A\)</span> is equal to the product of eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>. We will show this in a later notebook.</p></li>
<li><p>For a more comprehensive list visit <a class="reference external" href="https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant">Wikipedia</a></p></li>
</ol>
</div>
<div class="section" id="adjugate-matrix">
<h2>Adjugate matrix<a class="headerlink" href="#adjugate-matrix" title="Permalink to this headline">¶</a></h2>
<p>An <strong>adjugate</strong> of a square matrix <span class="math notranslate nohighlight">\(A\)</span> is the transpose of the cofactor matrix <span class="math notranslate nohighlight">\(C\)</span> of <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \text{adj}(A) = C^T, \]</div>
<p>where the entries of <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(C_{ij}\)</span> are cofactors calculated as described above. An adjugate matrix is useful for determining the inverse of small matrices with the following relation:</p>
<div class="math notranslate nohighlight">
\[ A^{-1} = \frac{\text{adj}(A)}{|A|}. \]</div>
<div class="section" id="example-inverse-of-a-2-times-2-matrix">
<h3>Example: Inverse of a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix<a class="headerlink" href="#example-inverse-of-a-2-times-2-matrix" title="Permalink to this headline">¶</a></h3>
<p>Let us find the inverse of the following matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{2 \times 2}\)</span> using the above formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split} A = \begin{pmatrix} 3 &amp; 1 \\ 4 &amp; 2 \end{pmatrix} \end{split}\]</div>
<p>The determinant is easily calculated: <span class="math notranslate nohighlight">\(|A| = 3 \times 2 - 1 \times 4 = 2\)</span>.</p>
<p>We calculate the cofactor matrix from <span class="math notranslate nohighlight">\(C_{ij} = (-1)^{i+j}M_{ij}\)</span>. For example, <span class="math notranslate nohighlight">\(C_{11} = M_{11}\)</span>, i.e. <span class="math notranslate nohighlight">\(M_{11}\)</span> is the determinant of <span class="math notranslate nohighlight">\(A\)</span> after removing the first row and column from it. Therefore, <span class="math notranslate nohighlight">\(M_{11} = |2| = 2\)</span>. Similarly for the other entries:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \text{adj}(A) = C^T = 
\begin{pmatrix} 2 &amp; -4 \\ -1 &amp; 3 \end{pmatrix}^T = 
\begin{pmatrix} 2 &amp; -1 \\ -4 &amp; 3 \end{pmatrix}.\end{split}\]</div>
<p>Plugging both back into the formula for the inverse:</p>
<div class="math notranslate nohighlight">
\[\begin{split} A^{-1} = \frac{1}{2} \begin{pmatrix} 2 &amp; -1 \\ -4 &amp; 3 \end{pmatrix} = \begin{pmatrix} 1 &amp; -1/2 \\ -2 &amp; 3/2 \end{pmatrix} \end{split}\]</div>
</div>
</div>
<div class="section" id="cramer-s-rule">
<h2>Cramer’s rule<a class="headerlink" href="#cramer-s-rule" title="Permalink to this headline">¶</a></h2>
<p>Let \(A = (a_1, a_2, …, a_n) \) be a square matrix (real or complex) such that \( \det A \neq 0\). Then for all <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>, the equation <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span> has a unique solution <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Furthermore, the coordinates <span class="math notranslate nohighlight">\(x_i\)</span> of the solution <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are given by the formula:</p>
<div class="math notranslate nohighlight">
\[ x_i = \frac{\det(a_1, ..., a_{i-1}, b, a_{i+1}, ..., a_n)}{\det(a_1, ..., a_n)}, \]</div>
<p>where we swapped the ith column of <span class="math notranslate nohighlight">\(A\)</span> with <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>.</p>
<div class="section" id="example-solving-a-system-of-complex-linear-equations">
<h3>Example: Solving a system of complex linear equations<a class="headerlink" href="#example-solving-a-system-of-complex-linear-equations" title="Permalink to this headline">¶</a></h3>
<p>Consider the following system of two equations and two unknowns:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 2ix - y = 2 - i \\
3x - iy = 2i \end{split}\]</div>
<p>which can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} A \mathbf{x} =
\begin{pmatrix} 2i &amp; -1 \\ 3 &amp; -i \end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix} = 
\begin{pmatrix} 2 - i \\ 2i \end{pmatrix} = \mathbf{b}. \end{split}\]</div>
<p>Let us find <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> using Cramer’s rule. To find <span class="math notranslate nohighlight">\(x\)</span> we need to substitute <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> in the 1st column of the matrix in the numerator:</p>
<div class="math notranslate nohighlight">
\[\begin{split} x = \frac{\left | \begin{array}{cc} 2 - i &amp; -1 \\ 2i &amp; -i \end{array} \right |}{\left | \begin{array}{cc} 2i &amp; -1 \\ 3 &amp; -i \end{array} \right |} 
= -\frac{1}{5}\end{split}\]</div>
<p>Now for <span class="math notranslate nohighlight">\(y\)</span> we need to substitute <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> in the 2nd column in the numerator:</p>
<div class="math notranslate nohighlight">
\[\begin{split} y = \frac{\left | \begin{array}{cc} 2i &amp; 2 - i \\ 3 &amp; 2i \end{array} \right |}{\left | \begin{array}{cc} 2i &amp; -1 \\ 3 &amp; -i \end{array} \right |} 
= \frac{-10 + 3i}{5} = -2 + \frac{3}{5}i\end{split}\]</div>
<p>Therefore, the solution <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a couple <span class="math notranslate nohighlight">\((-\frac{1}{5}, -2 + \frac{3}{5}i)\)</span>.</p>
</div>
</div>
<div class="section" id="vector-norms">
<h2>Vector norms<a class="headerlink" href="#vector-norms" title="Permalink to this headline">¶</a></h2>
<p>Simply put, vector <em>norms</em> are a measure of vector length. We will often represent a dataset in the form of a vector, so having some kind of scalar measure for vectors will be very useful in solving many kinds of problems (e.g. regression).</p>
<p>A norm is a function \( \lVert \cdot \rVert : \mathbb{C}^m \to \mathbb{R} \) such that for all \( \mathbf{x}, \mathbf{y} \in \mathbb{C}^m \) and \( \alpha \in \mathbb{C} \):</p>
<ol class="simple">
<li><p>\( \lVert \mathbf{x} \rVert \geq 0, \) with \( \lVert \mathbf{x} \rVert = 0 \) iff ( \mathbf{x} = \mathbf{0} \) (\(\mathbf{x}) is a <em>null-vector</em>),</p></li>
<li><p>\(\lVert \alpha \mathbf{x} \rVert = | \alpha | \lVert \mathbf{x} \rVert\) (scaling a vector scales its norm by the same amount),</p></li>
<li><p>\(\lVert \mathbf{x} + \mathbf{y} \rVert \leq \lVert \mathbf{x} \rVert + \lVert \mathbf{y} \rVert \) (triangle inequality).</p></li>
</ol>
<p>The most common norm is <strong>Euclidean norm</strong>, which gives the length of the vector as the Euclidean distance from the origin (magnitude). This is perhaps our intuitive idea of vector length, a consequence of Pythagorean theorem (in n dimensions). For a vector \( \mathbf{x} \in \mathbb{R}^n \), the Euclidean norm is given by a square of a dot product of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with itself, which is the inner product we saw before:</p>
<div class="math notranslate nohighlight">
\[ \lVert \mathbf{x} \rVert_2 = \sqrt{\mathbf{x}^T \mathbf{x}} = \sqrt{\sum\nolimits_{i=1}^n x_i^2}. \]</div>
<p>For a complex vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{C}^n\)</span>, we will often need to take a conjugate transpose - including now when defining Euclidean norm:</p>
<div class="math notranslate nohighlight">
\[ \lVert \mathbf{z} \rVert_2 = \sqrt{\mathbf{z}^* \mathbf{z}} = \sqrt{\overline{z}_1 z_1 + \overline{z}_2 z_2 + \cdots + \overline{z}_n z_n}. \]</div>
<p>This is because now each of the additive terms <span class="math notranslate nohighlight">\(\overline{z}_i z_i\)</span> is the squared modulus <span class="math notranslate nohighlight">\(|z_i|^2\)</span>, for <span class="math notranslate nohighlight">\(i = 1, ..., n\)</span>.</p>
<p>The Euclidean norm is also called the <span class="math notranslate nohighlight">\(L_2\)</span>-norm or simply 2-norm. It is part of a larger class of norms called <strong>p-norms</strong>. For a general vector \( \mathbf{x} \in \mathbb{C}^n \):</p>
<div class="math notranslate nohighlight">
\[ \lVert \mathbf{x} \rVert_p = \left( \sum_{i=1}^n | x_i |^p \right)^{1/p}, \qquad p \geq 1. \]</div>
<p>Let us also consider the <strong>taxicab norm</strong> (<span class="math notranslate nohighlight">\(p = 1\)</span>) and the <strong>infinity norm</strong> (\(\lim\limits_{p \to \infty} \lVert \mathbf{x} \rVert_p\)). The <span class="math notranslate nohighlight">\(L_1\)</span> norm is the sum of the absolute values:</p>
<div class="math notranslate nohighlight">
\[ \lVert \mathbf{x} \rVert_1 = \sum_{i=1}^n | x_i |, \]</div>
<p>while the infinity norm is simply the maximum absolute element:</p>
<div class="math notranslate nohighlight">
\[ \lVert \mathbf{x} \rVert_\infty = \max\limits_{i}|x_i|. \]</div>
<p>This is illustrated on the figure below, showing unit circles (set of all vectors of norm 1) in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> in different p-norms. That is, a vector from the origin to any point situated on the circle has a norm of 1 calculated in the corresponding p-norm. Modified from <a class="reference external" href="https://en.wikipedia.org/wiki/Norm_(mathematics)">Wikipedia</a>.</p>
<img src="linalgdata/norms.png">
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>Let us consider a vector \(\mathbf{x} = (3, -4)^T \). We can simply calculate the <span class="math notranslate nohighlight">\(L_1\)</span>, <span class="math notranslate nohighlight">\(L_2\)</span> and infinity norms:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\lVert \mathbf{x} \rVert_1 = |3| + |{-4}| = 7 \\
\lVert \mathbf{x} \rVert_2 = \sqrt{|3|^2 + |{-4}|^2} = 5 \\
\lVert \mathbf{x} \rVert_\infty = \max(|3|, |{-4}|) = 4.\end{split}\]</div>
<img src="linalgdata/vectornorms2.png" style="width: 200px;">
<p>While <span class="math notranslate nohighlight">\(\lVert \mathbf{x} \rVert_1\)</span> is defined as the unique Euclidean distance between two points, <span class="math notranslate nohighlight">\(\lVert \mathbf{x} \rVert_1\)</span> is not unique.</p>
</div>
</div>
<div class="section" id="matrix-norms">
<h2>Matrix norms<a class="headerlink" href="#matrix-norms" title="Permalink to this headline">¶</a></h2>
<p>We could think of a <span class="math notranslate nohighlight">\(\mathbb{C}^{m \times n}\)</span> matrix as a <span class="math notranslate nohighlight">\(\mathbb{C}^{mn}\)</span> vector and apply the usual vector norms.</p>
<p>For example, the <strong>Frobenius</strong> matrix norm is an analogue of the Euclidean norm for vectors:</p>
<div class="math notranslate nohighlight">
\[ \lVert A \rVert _F = \left ( \sum_{i=1}^{m} \sum_{j=1}^{n} | a_{ij} |^2 \right )^{1/2} \]</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "primer-computational-mathematics/book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./c_mathematics/linear_algebra"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="2_Linear_Systems.html" title="previous page">Systems of linear equations</a>
    <a class='right-next' id="next-link" href="4_Eigenvalues_and_eigenvectors.html" title="next page">Eigenvalues and eigenvectors</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Imperial College London<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>