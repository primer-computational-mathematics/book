

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Chapter 10 – General Back Propagation &#8212; ESE Jupyter Material</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".cell"
        const thebe_selector_input = ".cell_input div.highlight"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chapter 11 – Underfitting and Overfitting" href="Chapter 11 -- Underfitting and Overfitting.html" />
    <link rel="prev" title="Chapter 9 – Back Propagation" href="Chapter 9 -- Back Propagation.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ESE Jupyter Material</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../intro.html">Landing page</a>
  </li>
  <li class="">
    <a href="../../coding/intro.html">Coding</a>
  </li>
  <li class="active">
    <a href="../intro.html">Geosciences</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="../1D_Heat_Conduction.html">1D heat conduction (layered medium)</a>
    </li>
    <li class="">
      <a href="../1D_Resistivity_Forward_Modelling.html">1D resistivity forward modelling</a>
    </li>
    <li class="">
      <a href="../Climate_Model.html">Climate model</a>
    </li>
    <li class="">
      <a href="../Dynamic_Earth.html">Seafloor ages</a>
    </li>
    <li class="">
      <a href="../Keplerian_Orbits.html">Keplerian orbits</a>
    </li>
    <li class="">
      <a href="../Milankovitch_cycles.html">Milankovitch cycles</a>
    </li>
    <li class="">
      <a href="../Seismology.html">Downloading earthquake data</a>
    </li>
    <li class="">
      <a href="../Tides.html">Tides</a>
    </li>
    <li class="">
      <a href="../Wave_dispersion.html">Wave dispersion</a>
    </li>
    <li class="active">
      <a href="intro.html">Machine Learning</a>
      <ul class="nav sidenav_l3">
      <li class="">
        <a href="Chapter 0 -- Introduction.html">Chapter 0 – Introduction</a>
      </li>
      <li class="">
        <a href="Chapter 1 -- Neural Network.html">Chapter 1 – Neural Network</a>
      </li>
      <li class="">
        <a href="Chapter 2 -- Maximum Likelihood.html">Chapter 2 – Maximum Likelihood</a>
      </li>
      <li class="">
        <a href="Chapter 3 -- Cross Entropy.html">Chapter 3 – Cross Entropy</a>
      </li>
      <li class="">
        <a href="Chapter 4 -- Cost Function.html">Chapter 4 – Cost Function</a>
      </li>
      <li class="">
        <a href="Chapter 5 -- Gradient Descent 1.html">Chapter 5 – Gradient Descent 1</a>
      </li>
      <li class="">
        <a href="Chapter 6 -- Gradient Descent 2.html">Chapter 6 – Gradient Descent 2</a>
      </li>
      <li class="">
        <a href="Chapter 7 -- Real (Non-linear) Neural Network.html">Chapter 7 – Real (Non-linear) Neural Network</a>
      </li>
      <li class="">
        <a href="Chapter 8 -- Feedforward.html">Chapter 8 – Feedforward</a>
      </li>
      <li class="">
        <a href="Chapter 9 -- Back Propagation.html">Chapter 9 – Back Propagation</a>
      </li>
      <li class="active">
        <a href="">Chapter 10 – General Back Propagation</a>
      </li>
      <li class="">
        <a href="Chapter 11 -- Underfitting and Overfitting.html">Chapter 11 – Underfitting and Overfitting</a>
      </li>
      <li class="">
        <a href="Chapter 12 -- Early-stopping, Dropout & Mini-batch.html">Chapter 12 – Early-stopping, Dropout & Mini-batch</a>
      </li>
      <li class="">
        <a href="Chapter 13 -- Vanishing Gradient 1.html">Chapter 13 – Vanishing Gradient 1</a>
      </li>
      <li class="">
        <a href="Chapter 14 -- Vanishing Gradient 2.html">Chapter 14 – Vanishing Gradient 2</a>
      </li>
      <li class="">
        <a href="Chapter 15 -- Regularisation.html">Chapter 15 – Regularisation</a>
      </li>
      <li class="">
        <a href="Chapter 16 -- Other Activation Functions.html">Chapter 16 – Other Activation Functions</a>
      </li>
      <li class="">
        <a href="Chapter 17 -- Local Minima Trap.html">Chapter 17 – Local Minima Trap</a>
      </li>
      <li class="">
        <a href="Chapter 18 -- Softmax.html">Chapter 18 – Softmax</a>
      </li>
      <li class="">
        <a href="Chapter 19 -- Hyper-Parameters.html">Chapter 19 – Hyper-Parameters</a>
      </li>
    </ul>
    </li>
    <li class="">
      <a href="../remote_sensing/intro.html">Remote sensing</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../../mathematics/intro.html">Mathematics</a>
  </li>
  <li class="">
    <a href="../../modules/intro.html">Modules</a>
  </li>
  <li class="">
    <a href="../../z_extra_resources_for_researchers/intro.html">Extra resources for researchers</a>
  </li>
  <li class="">
    <a href="../../genindex.html">Index</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/geosciences/UROP_shapers/Chapter 10 -- General Back Propagation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/primer-computational-mathematics/book/issues/new?title=Issue%20on%20page%20%2Fgeosciences/UROP_shapers/Chapter 10 -- General Back Propagation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/primer-computational-mathematics/book/master?urlpath=tree/notebooks/geosciences/UROP_shapers/Chapter 10 -- General Back Propagation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="chapter-10-general-back-propagation">
<span id="general-back-propagation"></span><h1>Chapter 10 – General Back Propagation<a class="headerlink" href="#chapter-10-general-back-propagation" title="Permalink to this headline">¶</a></h1>
<img src="images/singleNeuronStructure3.PNG" width="800">
<center>Figure 1.14: The3blue1brown does a pretty good animation already:
    https://www.youtube.com/watch?v=tIeHLnjs5U8<p>To better understand the general format, let’s have even one more layer…four layers (figure 1.14). So we have one input layer, two hidden layers and one output layer. To simplify the problem, we have only one neuron in each layer (one weight per layer, e.g. <span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>,…), with <span class="math notranslate nohighlight">\(b=0\)</span>. And <span class="math notranslate nohighlight">\(C\)</span> is some cost function. The output in the output layer,</p>
<p>\begin{equation}
a^{(N)} = \boldsymbol{y} = \sigma(w^{(N)}*a^{(N-1)})
\end{equation}
where <span class="math notranslate nohighlight">\(N=3\)</span> in this example of figure 1.14.</p>
<p>Therefore we have</p>
<p>\begin{equation}
a^{(3)} = \boldsymbol{y} = \sigma(w^{(3)}*a^{(2)})
\end{equation}
where <span class="math notranslate nohighlight">\(a^{(2)} = \sigma(h_2)\)</span>, and it determines the cost function (simplified to <span class="math notranslate nohighlight">\(C=(\boldsymbol{y}-y)^2\)</span>). If the actual output <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> from the network is close to the desired output <span class="math notranslate nohighlight">\(y\)</span>, then the cost will be low, while if it’s far away, the cost will be high.</p>
<p>Also, let’s remember that we have used</p>
<p>\begin{equation}
h^{(N)} = w^{(N)}*a^{(N-1)}
\end{equation}</p>
<p>to make things simpler.</p>
<p>Note that I use <span class="math notranslate nohighlight">\(w_n\)</span> and <span class="math notranslate nohighlight">\(w^{(N)}\)</span> interchangeably, but they are the same in the single neuron net example. The notation of <span class="math notranslate nohighlight">\(w^{(N)}\)</span> is for a more complicated network we cover later. Anyway, let’s study the gradient <span class="math notranslate nohighlight">\(\frac{\delta C}{\delta w_1}\)</span> associated to the first hidden neuron. If we change the weight <span class="math notranslate nohighlight">\(w_1\)</span> by a tiny nudge <span class="math notranslate nohighlight">\(\Delta w_1\)</span>. That will set off a cascading series of changes in the rest of the network. First, it causes a change <span class="math notranslate nohighlight">\(\Delta a_1\)</span> (<span class="math notranslate nohighlight">\(a_1 = \sigma(h_1)\)</span>) in the output from the first hidden neuron. That, in turn, will cause a change <span class="math notranslate nohighlight">\(\Delta h_2\)</span> in the weighted input to the second hidden neuron. Then a change <span class="math notranslate nohighlight">\(\Delta a_2\)</span> in the output from the second hidden neuron. And so on, all the way through to a change <span class="math notranslate nohighlight">\(\Delta C\)</span> in the cost at the output. We have</p>
<p>\begin{equation}
\frac{\delta C}{\delta w_1} \approx \frac{\Delta C}{\Delta w_1}
\end{equation}</p>
<p>This suggests that we can figure out an expression for the gradient <span class="math notranslate nohighlight">\(\frac{\delta C}{\delta w_1}\)</span> by carefully tracking the effect of each step in this cascade. To do this, let’s think about how <span class="math notranslate nohighlight">\(\Delta w_1\)</span> causes the output a1 from the first hidden neuron to change. We have <span class="math notranslate nohighlight">\(a_1=\sigma(h1)=\sigma(w_1*x)\)</span>, so the change in <span class="math notranslate nohighlight">\(a_1\)</span></p>
<p>\begin{equation}
\Delta a_1 \approx \frac{\delta \sigma(h_1)}{\delta w_1} * \Delta w_1 =\frac{\delta \sigma(h_1)}{\delta h_1}<em>\frac{\delta h_1}{\delta w_1}</em>\Delta w_1 =   \sigma^{‘}(h_1)<em>x</em>\Delta w_1
\end{equation}</p>
<p>That <span class="math notranslate nohighlight">\(\sigma^{'}(h_1)\)</span> term converts a change <span class="math notranslate nohighlight">\(\Delta w_1\)</span> in the weight into a change <span class="math notranslate nohighlight">\(\Delta a_1\)</span> in the output activation. That change <span class="math notranslate nohighlight">\(\Delta a_1\)</span> in turn causes a change in the weighted input <span class="math notranslate nohighlight">\(h_2=w_2*a_1\)</span> to the second hidden neuron:</p>
<p>\begin{equation}
\Delta h_2 \approx \frac{\delta h_2}{\delta a_1} * \Delta a_1 = w_2*\Delta a_1
\end{equation}</p>
<p>Combining our expressions for <span class="math notranslate nohighlight">\(\Delta h_2\)</span> and <span class="math notranslate nohighlight">\(\Delta a_1\)</span>, we see how the change in the weight <span class="math notranslate nohighlight">\(w_1\)</span> propagates along the network to affect <span class="math notranslate nohighlight">\(h_2\)</span>:</p>
<p>\begin{equation}
\Delta h_2 \approx w_2* \sigma^{‘}(h_1)<em>x</em>\Delta w_1
\end{equation}</p>
<p>This change in <span class="math notranslate nohighlight">\(h_2\)</span> can result in a change in <span class="math notranslate nohighlight">\(a_2 = \sigma(h2)\)</span>:</p>
<p>\begin{equation}
\Delta a_2 = \Delta \sigma(h_2) \approx \frac{\delta a_2}{h_2} * \Delta h_2 =  \sigma^{‘}(h_2) * \Delta h_2 =\sigma^{‘}(h_2) * w_2* \sigma^{‘}(h_1)*\Delta w_1
\end{equation}
which will cause a change in <span class="math notranslate nohighlight">\(h_3\)</span>.</p>
<p>\begin{equation}
\Delta h_3 \approx \frac{\delta h_3}{\delta a_2} * \Delta a_2 = w_3 * \Delta a_2 = w_3 * \sigma^{‘}(h_2) * w_2* \sigma^{‘}(h_1)<em>x</em>\Delta w_1
\end{equation}</p>
<p>This change in <span class="math notranslate nohighlight">\(h_3\)</span> will cause a change in <span class="math notranslate nohighlight">\(\sigma(h_3)\)</span>, also known as <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, the prediction result.</p>
<p>\begin{equation}
\Delta \boldsymbol{y} = \Delta \sigma(h_3) \approx \frac{\delta y}{h_3} * \Delta h_3 =\sigma^{‘}(h_3) * \Delta h_3 = \sigma^{‘}(h_3) * w_3 * \sigma^{‘}(h_2) * w_2* \sigma^{‘}(h_1)<em>x</em>\Delta w_1
\end{equation}</p>
<p>This change in the prediction value <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> will cause a change in the cost function, <span class="math notranslate nohighlight">\(\Delta C\)</span>, making it bigger or smaller (of course we want it to be as small as possible).</p>
<p>\begin{equation}
\Delta C \approx \frac{\delta C}{\delta \boldsymbol{y}} * \Delta \boldsymbol{y} = \frac{\delta C}{\delta \boldsymbol{y}}<em>\sigma^{‘}(h_3) * w_3 * \sigma^{‘}(h_2) * w_2</em> \sigma^{‘}(h_1)<em>x</em>\Delta w_1
\label{deltaC}
\end{equation}</p>
<p>We can keep going in this fashion, tracking the way changes propagate through the rest of the network, in theory to the <span class="math notranslate nohighlight">\(n^{th}\)</span> layer. After dividing <span class="math notranslate nohighlight">\(\Delta w_1\)</span> at the both sides in equation \ref{deltaC}, the resulting gradient for the <span class="math notranslate nohighlight">\(w_1\)</span> in this four layers network should be:</p>
<p>\begin{equation}
\frac{\delta C}{\delta w_1} =  x*\sigma^{‘}(h_1)<em>w_2</em>\sigma^{‘}(h_2)<em>w_3</em>\sigma^{‘}(h_3)*\frac{\delta C}{\delta \boldsymbol{y}}
\label{dCdw1}
\end{equation}</p>
<p>Or</p>
<p>\begin{equation}
\frac{\delta C}{\delta w_1} = \frac{\delta h_1}{\delta w_1}<em>\frac{\delta a^{(1)}}{\delta h_1}</em>\frac{\delta h_2}{\delta a^{(1)}}<em>\frac{\delta a^{(2)}}{\delta h_2}</em>\frac{\delta h_3}{\delta a^{(2)}}<em>\frac{\delta a^{(3)}}{\delta h_3}</em>\frac{\delta C}{\delta a^{(3)}}
\end{equation}</p>
<p>We should be reaffirming our idea of defining the `Error Term’ (by figure 1.15) for the efficiency of our code.</p>
<img src="images/singleNeuronStructure4.PNG" width="800">
<center> Figure 1.15<p>If we take a look at just the last two neurons – <span class="math notranslate nohighlight">\(a^{(N-1)}\)</span> and <span class="math notranslate nohighlight">\(a^{(N)}\)</span>, then we have the following:</p>
<p>\begin{equation}
\frac{\delta h_3}{\delta a^{(2)}}<em>\frac{\delta a^{(3)}}{\delta h_3}</em>\frac{\delta C}{\delta a^{(3)}}
\end{equation}</p>
<p>which is investigating the amount of change of <span class="math notranslate nohighlight">\(C\)</span> due to a tiny nudge of <span class="math notranslate nohighlight">\(a^{(2)}\)</span>.</p>
<p>This is helpful because now we can just keep iterating this chain rule idea backwards to see how sensitive the cost to the previous weights and biases.</p>
<p>This is just a little bit more complicated when we have more than one neuron in each layer because we have more indices to keep track of. Let’s take a look at the last two layers in a multiple neuron structure.</p>
<img src="images/multipleNeuronStructure1.PNG" width="800">
<center> Figure 1.16<p>The change of <span class="math notranslate nohighlight">\(h_1^{(3)}\)</span> or <span class="math notranslate nohighlight">\(h_2^{(3)}\)</span> in the output layer will change the cost <span class="math notranslate nohighlight">\(C\)</span>, and we define such gradient as <span class="math notranslate nohighlight">\(\delta^{(N)}_j\)</span>:</p>
<p>\begin{equation}
\frac{\delta C}{\delta h_1^{(3)}} = \delta^{(3)}_1
\end{equation}</p>
<p>and</p>
<p>\begin{equation}
\frac{\delta C}{\delta h_2^{(3)}} = \delta^{(3)}_2
\end{equation}</p>
<p>or in general</p>
<p>\begin{equation}
\frac{\delta C}{\delta h_j^{(N)}} = \delta^{(N)}_j = \frac{\delta C}{\delta a^{(N)}_j} \frac{a^{(N)}_j}{\delta h_j^{(N)}} = \frac{\delta C}{\delta a^{(N)}_j} \sigma^{‘}(h_j^{(N)})
\end{equation}</p>
<p>It’s easy to rewrite the equation in a matrix-based form, as</p>
<p>\begin{equation}
\delta^{(N)} = \frac{\delta C}{\delta a^{(N)}}\odot \sigma^{‘}(h^{(N)})
\end{equation}</p>
<p>Then, the change in the <span class="math notranslate nohighlight">\(h_j^{(N-1)}\)</span> in the <span class="math notranslate nohighlight">\(N-1\)</span> layer will change <span class="math notranslate nohighlight">\(a^{(N-1)}_j\)</span> and then results in a change in <span class="math notranslate nohighlight">\(a^{(N)}_j\)</span>, which will cause the change in the cost <span class="math notranslate nohighlight">\(C\)</span>. In specific (red neuron in figure 1.16), we have</p>
<p>\begin{equation}
\frac{\delta C}{\delta h_j^{(N-1)}} = \frac{\delta C}{\delta h_2^{(2)}} = \frac{\delta C}{\delta a^{(3)}_1} * \frac{\delta a^{(3)}_1}{\delta h_1^{(3)}} * \frac{\delta h_1^{(3)}}{\delta a^{(2)}_2}<em>\frac{\delta a^{(2)}_2}{\delta h_2^{(2)}} +  \frac{\delta C}{\delta a^{(3)}_2} * \frac{\delta a^{(3)}_2}{\delta h_2^{(3)}} * \frac{\delta h_2^{(3)}}{\delta a^{(2)}_2}</em>\frac{\delta a^{(2)}_2}{\delta h_2^{(2)}}
\label{dCdh22}
\end{equation}</p>
<p>Similarly,</p>
<p>\begin{equation}
\frac{\delta C}{\delta h_1^{(2)}} = \frac{\delta C}{\delta a^{(3)}_1} * \frac{\delta a^{(3)}_1}{\delta h_1^{(3)}} * \frac{\delta h_1^{(3)}}{\delta a^{(2)}_1}<em>\frac{\delta a^{(2)}_1}{\delta h_1^{(2)}} +  \frac{\delta C}{\delta a^{(3)}_2} * \frac{\delta a^{(3)}_2}{\delta h_2^{(3)}} * \frac{\delta h_2^{(3)}}{\delta a^{(2)}_1}</em>\frac{\delta a^{(2)}_1}{\delta h_1^{(2)}}
\label{dCdh12}
\end{equation}</p>
<p>and</p>
<p>\begin{equation}
\frac{\delta C}{\delta h_3^{(2)}} = \frac{\delta C}{\delta a^{(3)}_1} * \frac{\delta a^{(3)}_1}{\delta h_1^{(3)}} * \frac{\delta h_1^{(3)}}{\delta a^{(2)}_3}<em>\frac{\delta a^{(2)}_3}{\delta h_3^{(2)}} +  \frac{\delta C}{\delta a^{(3)}_2} * \frac{\delta a^{(3)}_2}{\delta h_2^{(3)}} * \frac{\delta h_2^{(3)}}{\delta a^{(2)}_3}</em>\frac{\delta a^{(2)}_3}{\delta h_3^{(2)}}
\label{dCdh32}
\end{equation}</p>
<img src="images/ErrorTermN_1.PNG" width="800">
<center> Figure 1.17<p>Now, we can define such gradient in equations \ref{dCdh22}, \ref{dCdh12} and \ref{dCdh32} by <span class="math notranslate nohighlight">\(\delta^{(N-1)}\)</span> in a vector form with <span class="math notranslate nohighlight">\(\delta^{(N)}\)</span> (shown in figure 1.17)</p>
<p>\begin{equation}
\begin{bmatrix} \delta_1^{(2)}\delta_2^{(2)}\delta_3^{(2)}\end{bmatrix}
= \begin{bmatrix} w^{(3)}<em>{11} &amp; w^{(3)}</em>{21} \w^{(3)}<em>{12} &amp; w^{(3)}</em>{23} \w^{(3)}<em>{13} &amp; w^{(3)}</em>{23} \end{bmatrix}
\cdot
\begin{bmatrix} \delta_1^{(3)}\delta_2^{(3)}\end{bmatrix}
\odot
\begin{bmatrix} \sigma^{‘}(h_1^{(2)})\sigma^{‘}(h_2^{(2)})\sigma^{‘}(h_3^{(2)})\end{bmatrix}
\end{equation}</p>
<p>This in general is the following</p>
<p>\begin{equation}
\delta^{(N-1)} = w^{(N)} \cdot \delta^{(N)} \odot \sigma^{‘}_{h^{N-1}}
\label{ErrorTermN_1_general}
\end{equation}
where <span class="math notranslate nohighlight">\(\odot\)</span> is the handamard or elementwise product.</p>
<p>We can keep going back (to the left) in the neural network to investigate how the change in <span class="math notranslate nohighlight">\(\sigma_3^{N-2}\)</span> (i.e. <span class="math notranslate nohighlight">\(\sigma_3^{1}\)</span>) can cause a change in <span class="math notranslate nohighlight">\(C\)</span>.</p>
<img src="images/multipleNeuronStructure2.PNG" width="800">
<center> Figure 1.18<p>For example, the change of neuron in red circle causes changes of three neurons in the second layer, and then they cause changes in the two neurons the output layer, which change the cost. We also need to consider the change of cost due to the second neuron (i.e. <span class="math notranslate nohighlight">\(\sigma_1(h_2)\)</span>). To express this in a general form:</p>
<p>\begin{equation}
\begin{bmatrix} \delta_1^{(1)}\delta_2^{(1)}\end{bmatrix}
= \begin{bmatrix} w^{(2)}<em>{11} &amp; w^{(2)}</em>{21} &amp; w^{(2)}<em>{31} \w^{(2)}</em>{12} &amp; w^{(2)}<em>{22} &amp; w^{(2)}</em>{32} \end{bmatrix}
\cdot
\begin{bmatrix} \delta_1^{(2)}\delta_2^{(2)}\delta_3^{(2)}\end{bmatrix}
\odot
\begin{bmatrix} \sigma^{‘}(h_1^{(1)})\sigma^{‘}(h_2^{(1)})\end{bmatrix}
\label{dCdh_1}
\end{equation}</p>
<p>So we have obtained the change of <span class="math notranslate nohighlight">\(C\)</span> with respect to <span class="math notranslate nohighlight">\(h_i\)</span> in the first layer. Now let us remember the actual goal here is to investigate how the change in weights caused the change in the cost. That’s right, it is the change of weights <span class="math notranslate nohighlight">\(w\)</span> that changes the <span class="math notranslate nohighlight">\(h_i\)</span>. This connects with the expression in equation \ref{dCdh_1}: <span class="math notranslate nohighlight">\(\frac{\delta C}{\delta h_i^{1}}\)</span>, which leads to the change of <span class="math notranslate nohighlight">\(C\)</span>.</p>
<img src="images/multipleNeuronStructure4.PNG" width="800">
<center> Figure 1.19<p>For example, since we know that</p>
<p>\begin{equation}
h^{(1)}<em>1=w</em>{11}*x_1 + w_{12}*x_2 + w_{13}*x_3
\end{equation}</p>
<p>Then its derivative wrt. <span class="math notranslate nohighlight">\(w_{11}\)</span> (i.e. <span class="math notranslate nohighlight">\(\frac{\delta h^{(1)}_1}{\delta w_{11}}\)</span>) is just <span class="math notranslate nohighlight">\(x_1\)</span>.</p>
<p>Therefore, we can use the chain rule to obtain the change of <span class="math notranslate nohighlight">\(C\)</span> due to the change of <span class="math notranslate nohighlight">\(w_{11}\)</span></p>
<p>\begin{equation}
\frac{\delta C}{\delta w_{11}}=\frac{\delta C}{\delta h_{1}^{(1)}}\frac{\delta h_{1}^{(1)}}{\delta w_{11}}=\delta_1^{(1)}*x_1
\end{equation}</p>
<p>Finally, we can update the weight by using the equation 20 in chapter 9.</p>
<p>Having understood backpropagation in the abstract, we can now understand the code used to implement backpropagation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return a tuple ``(nabla_b, nabla_w)`` representing the</span>
<span class="sd">    gradient for the cost function C_x.  ``nabla_b`` and</span>
<span class="sd">    ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span>
<span class="sd">    to ``self.biases`` and ``self.weights``.&quot;&quot;&quot;</span>
    <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
    <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>
    <span class="c1"># feedforward</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="c1"># list to store all the activations, layer by layer</span>
    <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list to store all the z vectors, layer by layer</span>
    <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
        <span class="n">zs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    <span class="c1"># backward pass</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
        <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
    <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
    <span class="c1"># Note that the variable l in the loop below is used a little</span>
    <span class="c1"># differently to the notation in Chapter 2 of the book.  Here,</span>
    <span class="c1"># l = 1 means the last layer of neurons, l = 2 is the</span>
    <span class="c1"># second-last layer, and so on.  It&#39;s a renumbering of the</span>
    <span class="c1"># scheme in the book, used here to take advantage of the fact</span>
    <span class="c1"># that Python can use negative indices in lists.</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
        <span class="n">sp</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sp</span>
        <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
        <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "primer-computational-mathematics/book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./geosciences/UROP_shapers"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Chapter 9 -- Back Propagation.html" title="previous page">Chapter 9 – Back Propagation</a>
    <a class='right-next' id="next-link" href="Chapter 11 -- Underfitting and Overfitting.html" title="next page">Chapter 11 – Underfitting and Overfitting</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Imperial College London<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>