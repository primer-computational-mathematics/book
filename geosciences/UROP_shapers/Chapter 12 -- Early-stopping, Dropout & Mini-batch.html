

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Chapter 12 – Early-stopping, Dropout &amp; Mini-batch &#8212; ESE Jupyter Material</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".cell"
        const thebe_selector_input = ".cell_input div.highlight"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chapter 13 – Vanishing Gradient 1" href="Chapter 13 -- Vanishing Gradient 1.html" />
    <link rel="prev" title="Chapter 11 – Underfitting and Overfitting" href="Chapter 11 -- Underfitting and Overfitting.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ESE Jupyter Material</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../intro.html">Landing page</a>
  </li>
  <li class="">
    <a href="../../coding/intro.html">Coding</a>
  </li>
  <li class="active">
    <a href="../intro.html">Geosciences</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="../1D_Heat_Conduction.html">1D heat conduction (layered medium)</a>
    </li>
    <li class="">
      <a href="../1D_Resistivity_Forward_Modelling.html">1D resistivity forward modelling</a>
    </li>
    <li class="">
      <a href="../Climate_Model.html">Climate model</a>
    </li>
    <li class="">
      <a href="../Dynamic_Earth.html">Seafloor ages</a>
    </li>
    <li class="">
      <a href="../Keplerian_Orbits.html">Keplerian orbits</a>
    </li>
    <li class="">
      <a href="../Milankovitch_cycles.html">Milankovitch cycles</a>
    </li>
    <li class="">
      <a href="../Seismology.html">Downloading earthquake data</a>
    </li>
    <li class="">
      <a href="../Tides.html">Tides</a>
    </li>
    <li class="">
      <a href="../Wave_dispersion.html">Wave dispersion</a>
    </li>
    <li class="active">
      <a href="intro.html">Machine Learning</a>
      <ul class="nav sidenav_l3">
      <li class="">
        <a href="Chapter 0 -- Introduction.html">Chapter 0 – Introduction</a>
      </li>
      <li class="">
        <a href="Chapter 1 -- Neural Network.html">Chapter 1 – Neural Network</a>
      </li>
      <li class="">
        <a href="Chapter 2 -- Maximum Likelihood.html">Chapter 2 – Maximum Likelihood</a>
      </li>
      <li class="">
        <a href="Chapter 3 -- Cross Entropy.html">Chapter 3 – Cross Entropy</a>
      </li>
      <li class="">
        <a href="Chapter 4 -- Cost Function.html">Chapter 4 – Cost Function</a>
      </li>
      <li class="">
        <a href="Chapter 5 -- Gradient Descent 1.html">Chapter 5 – Gradient Descent 1</a>
      </li>
      <li class="">
        <a href="Chapter 6 -- Gradient Descent 2.html">Chapter 6 – Gradient Descent 2</a>
      </li>
      <li class="">
        <a href="Chapter 7 -- Real (Non-linear) Neural Network.html">Chapter 7 – Real (Non-linear) Neural Network</a>
      </li>
      <li class="">
        <a href="Chapter 8 -- Feedforward.html">Chapter 8 – Feedforward</a>
      </li>
      <li class="">
        <a href="Chapter 9 -- Back Propagation.html">Chapter 9 – Back Propagation</a>
      </li>
      <li class="">
        <a href="Chapter 10 -- General Back Propagation.html">Chapter 10 – General Back Propagation</a>
      </li>
      <li class="">
        <a href="Chapter 11 -- Underfitting and Overfitting.html">Chapter 11 – Underfitting and Overfitting</a>
      </li>
      <li class="active">
        <a href="">Chapter 12 – Early-stopping, Dropout & Mini-batch</a>
      </li>
      <li class="">
        <a href="Chapter 13 -- Vanishing Gradient 1.html">Chapter 13 – Vanishing Gradient 1</a>
      </li>
      <li class="">
        <a href="Chapter 14 -- Vanishing Gradient 2.html">Chapter 14 – Vanishing Gradient 2</a>
      </li>
      <li class="">
        <a href="Chapter 15 -- Regularisation.html">Chapter 15 – Regularisation</a>
      </li>
      <li class="">
        <a href="Chapter 16 -- Other Activation Functions.html">Chapter 16 – Other Activation Functions</a>
      </li>
      <li class="">
        <a href="Chapter 17 -- Local Minima Trap.html">Chapter 17 – Local Minima Trap</a>
      </li>
      <li class="">
        <a href="Chapter 18 -- Softmax.html">Chapter 18 – Softmax</a>
      </li>
      <li class="">
        <a href="Chapter 19 -- Hyper-Parameters.html">Chapter 19 – Hyper-Parameters</a>
      </li>
    </ul>
    </li>
    <li class="">
      <a href="../remote_sensing/intro.html">Remote sensing</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../../mathematics/intro.html">Mathematics</a>
  </li>
  <li class="">
    <a href="../../modules/intro.html">Modules</a>
  </li>
  <li class="">
    <a href="../../z_extra_resources_for_researchers/intro.html">Extra resources for researchers</a>
  </li>
  <li class="">
    <a href="../../genindex.html">Index</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/geosciences/UROP_shapers/Chapter 12 -- Early-stopping, Dropout & Mini-batch.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/primer-computational-mathematics/book/issues/new?title=Issue%20on%20page%20%2Fgeosciences/UROP_shapers/Chapter 12 -- Early-stopping, Dropout & Mini-batch.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/primer-computational-mathematics/book/master?urlpath=tree/notebooks/geosciences/UROP_shapers/Chapter 12 -- Early-stopping, Dropout & Mini-batch.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#early-stopping" class="nav-link">Early-stopping</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#dropout" class="nav-link">Dropout</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#mini-batch" class="nav-link">Mini-batch</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="chapter-12-early-stopping-dropout-mini-batch">
<span id="underfitting-and-overfitting-2"></span><h1>Chapter 12 – Early-stopping, Dropout &amp; Mini-batch<a class="headerlink" href="#chapter-12-early-stopping-dropout-mini-batch" title="Permalink to this headline">¶</a></h1>
<div class="section" id="early-stopping">
<h2>Early-stopping<a class="headerlink" href="#early-stopping" title="Permalink to this headline">¶</a></h2>
<img src="images/earlyStopping.PNG" width="700">
<center> Figure 1.22:  Should early stop at epoch = 20 because the error in the testing set begins torise after that.<p>One of the main reason for overfitting is that the student studies too hard (too much epoch/iteration). So we need to let the program know that it needs to stop when it is about to overfit. If everything goes well, then the cost should be decreasing with more epoch/iterations. However, after certain epoch, the accuracy begins to drop, then we should stop at this iteration as more iterations would result in overfitting. This is known as the Early Stopping. A simple method to evaluate the underfitting and overfitting is shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the number of test inputs for which the neural</span>
<span class="sd">    network outputs the correct result. Note that the neural</span>
<span class="sd">    network&#39;s output is assumed to be the index of whichever</span>
<span class="sd">    neuron in the final layer has the highest activation.&quot;&quot;&quot;</span>
    <span class="n">test_results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
                    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">test_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dropout">
<h2>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h2>
<p>In the neural net, we have lots of neurons. If those neurons keep working without taking a rest, then it is highly likely to make the model overfit. Thus, we choose to drop out some neurons and let it rest, while keeping the others working, and do it over and over again over iterations with different sets of neurons.</p>
<img src="images/dropout.PNG" width="600">
<center> Figure 1.23<p>There are different ways of understanding how this approach helps to resolve the problem of overfitting. In philosophy, the drop out increases the diversity of the net. With the `new blood’ from the other neurons, each set of weights are different and are therefore more likely to perform better. The other explanation is that each neuron learns certain features in the data set, if lots of neurons learn a particular feature at the same time, then it would not be performing better than single individual neuron who is in charge of its own feature and then many of those neurons combined.</p>
<p>A related heuristic explanation for dropout is given in one of the earliest papers to use the technique: “This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.” In other words, if we think of our network as a model which is making predictions, then we can think of dropout as a way of making sure that the model is robust to the loss of any individual piece of evidence. In this, it’s somewhat similar to L1 and L2 regularization, which tend to reduce weights, and thus make the network more robust to losing any individual connection in the network.</p>
</div>
<div class="section" id="mini-batch">
<h2>Mini-batch<a class="headerlink" href="#mini-batch" title="Permalink to this headline">¶</a></h2>
<p>There are a number of challenges in applying the gradient descent rule. To understand what the problem is, let’s look back at the cost equation 11 in chapter Gradient Descent 2. Notice that this cost function has the form of summation, that is, it’s an average over costs or individual training examples (e.g. the average of all four students). In practice, to compute the gradient <span class="math notranslate nohighlight">\(\nabla C\)</span> we need to compute the gradients of <span class="math notranslate nohighlight">\(\nabla C_x\)</span> separately for each training input, <span class="math notranslate nohighlight">\(x\)</span>, and then average them. Unfortunately, when the number of training inputs is very large this can take a long time, and learning thus occurs slowly.</p>
<p>An idea called stochastic gradient descent can be used to speed up learning. The idea is to estimate the gradient <span class="math notranslate nohighlight">\(\nabla C\)</span> by computing <span class="math notranslate nohighlight">\(\nabla C_x\)</span> for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient <span class="math notranslate nohighlight">\(\nabla C\)</span>, and this helps speed up gradient descent, and thus learning.</p>
<p>To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number <span class="math notranslate nohighlight">\(m\)</span> of randomly chosen training inputs. We’ll label those random training inputs <span class="math notranslate nohighlight">\(X_1\)</span>,<span class="math notranslate nohighlight">\(X_2\)</span>,…,<span class="math notranslate nohighlight">\(X_m\)</span>, and refer to them as a mini-batch. Provided the sample size <span class="math notranslate nohighlight">\(m\)</span> is large enough we expect that the average value of the <span class="math notranslate nohighlight">\(\nabla C_{X_j}\)</span> will be roughly equal to the average over all <span class="math notranslate nohighlight">\(\nabla C_x\)</span>, that is</p>
<p>\begin{equation}
\frac{\sum^m_{j=1\nabla C_{X_j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C
\end{equation}
where the second sum is over the entire set of training data.</p>
<p>Swapping sides we get</p>
<p>\begin{equation}
\nabla C \approx \frac{\sum^m_{j=1\nabla C_{X_j}}}{m}
\end{equation}</p>
<p>confirming that we can estimate the overall gradient by computing gradients just for the randomly chosen mini-batch.</p>
<p>To connect this explicitly to learning in neural networks, suppose <span class="math notranslate nohighlight">\(w_k\)</span> and <span class="math notranslate nohighlight">\(b_l\)</span> denote the weights and biases in our neural network. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those,</p>
<p>\begin{equation}
w_i^{‘} = w_i - \eta \nabla C  =  w_i - \eta \frac{1}{m} \sum_{i=1}^{m_{j}} \frac{\delta C_{X_j}}{\delta w_k}
\end{equation}</p>
<p>where the sums are over all the training examples <span class="math notranslate nohighlight">\(X_j\)</span> in the current mini-batch. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we’ve exhausted the training inputs, which is said to complete an epoch of training. At that point we start over with a new training epoch.</p>
<p>We can think of stochastic gradient descent as being like political polling: it’s much easier to sample a small mini-batch than it is to apply gradient descent to the full batch, just as carrying out a poll is easier than running a full election. For example, if we have a training set of size <span class="math notranslate nohighlight">\(n=60,000\)</span>, as in MNIST, and choose a mini-batch size of (say) <span class="math notranslate nohighlight">\(m=10\)</span>, this means we’ll get a factor of <span class="math notranslate nohighlight">\(6,000\)</span> speedup in estimating the gradient!
Another example would be instead of calculating the average cost gradient for all four students, calculate three students and pray that those randomly chosen three students can represent the four students on average. Of course, the estimate won’t be perfect - there will be statistical fluctuations - but it doesn’t need to be perfect: all we really care about is moving in a general direction that will help decrease <span class="math notranslate nohighlight">\(C\)</span>, and that means we don’t need an exact computation of the gradient. In practice, stochastic gradient descent is a commonly used and powerful technique for learning in neural networks, and it’s the basis for most of the learning techniques we’ll develop in this book.</p>
<p>Recall from the chapter backpropagation that the code of backprop method was contained in the update_mini_batch of the Network class. In particular, the update_mini_batch method updates the Network’s weights and biases by computing the gradient for the current mini_batch of training examples:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="o">...</span>
    <span class="k">def</span> <span class="nf">update_mini_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the network&#39;s weights and biases by applying</span>
<span class="sd">        gradient descent using backpropagation to a single mini batch.</span>
<span class="sd">        The &quot;mini_batch&quot; is a list of tuples &quot;(x, y)&quot;, and &quot;eta&quot;</span>
<span class="sd">        is the learning rate.&quot;&quot;&quot;</span>
        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
            <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">nb</span><span class="o">+</span><span class="n">dnb</span> <span class="k">for</span> <span class="n">nb</span><span class="p">,</span> <span class="n">dnb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_b</span><span class="p">)]</span>
            <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">nw</span><span class="o">+</span><span class="n">dnw</span> <span class="k">for</span> <span class="n">nw</span><span class="p">,</span> <span class="n">dnw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_w</span><span class="p">,</span> <span class="n">delta_nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nw</span> 
                        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">nw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nb</span> 
                       <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">nabla_b</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "primer-computational-mathematics/book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./geosciences/UROP_shapers"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Chapter 11 -- Underfitting and Overfitting.html" title="previous page">Chapter 11 – Underfitting and Overfitting</a>
    <a class='right-next' id="next-link" href="Chapter 13 -- Vanishing Gradient 1.html" title="next page">Chapter 13 – Vanishing Gradient 1</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Imperial College London<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>